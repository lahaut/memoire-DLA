\documentclass[a4paper,11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage[a4paper]{geometry}

\newtheorem{The}{Théorème}[section]
\newtheorem{Lemme}[The]{Lemme}
\newtheorem{Prop}[The]{Proposition}
\newtheorem{Cor}[The]{Corollaire}
\theoremstyle{definition}
\newtheorem{Def}[The]{Définition}
\newtheorem{Ex}[The]{Exemple}
\theoremstyle{remark}
\newtheorem*{Rq}{Remarque}

\title{Un modèle de croissance aléatoire : le monde découvert par N explorateurs}
\author{Paul MELOTTI et Alexis PRÉVOST \\ sous la direction de Pierre BERTIN}
\date{\today}
\makeatletter
\renewcommand\theequation{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}
\makeatother

\begin{document}
\maketitle
\section*{Introduction}
Nous allons étudier dans ce mémoire l'évolution du monde découvert par des explorateurs dans le cas discret. Le résultat principal est celui de \cite{ref} qui montre qu'asymptotiquement celui-ci est une boule. Le modèle est le suivant : on lance N explorateurs sur $\mathbb{Z}^d$ les uns après les autres. On définit la région explorée récursivement de la façon suivante: on lance le $k^{\mathrm{\grave{e}me}}$ explorateur qui effectue une marche aléatoire symétrique et s'arrête dès qu'il tombe sur une case non explorée (on dit qu'il s'installe). La région explorée à l'instant $k$ est alors la région explorée à l'instant $k-1$ plus la position du $k^{\mathrm{\grave{e}me}}$ explorateur installé. A. Asselah et A. Gaudillière ont montré dans \cite{ref} que la région explorée tendait vers une boule, avec une erreur presque sûrement inférieure à $\alpha\sqrt{\log(n)}$ en dimension 3 et $\alpha \log(n)$ en dimension 2.

Nous allons travailler ici sur l'article \cite{ref2} d'A. Gaudillière et A. Asselah qui essaie d'établir l'optimalité des majorations précédentes. Il montre en particulier que $\sqrt{ \log (n)}$ est optimal en dimension supérieure à 3 pour l'erreur interne, mais ne conclut pas pour la dimension 2. En ce qui concerne l'erreur externe, ils prouvent un résultat qui permet de penser que $\sqrt{ \log (n)}$ est optimal en dimension supérieure à 3 sans tout à fait permettre de conclure.

\begin{center}
 \includegraphics[scale=0.4]{100000.png}
\end{center}


\newpage
\tableofcontents
\newpage

\section{Historique}
Le modèle qui nous intéresse est un exemple de modèle de croissance aléatoire couramment nommé IDLA (Internal Diffusion Limited Aggregation). Il a été introduit en 1986 par P. Meakin et J.M. Deutch \cite{chimie} dans le cadre de la chimie théorique pour décrire des processus comme l'électro-polissage. Dès l'introduction du modèle, il a semblé important de connaître la forme asymptotique du cluster obtenu, mais aussi l'ordre des variation par rapport à cette forme limite puisqu'il s'agissait de polir des matériaux.

P. Diaconis et W. Fulton ont formalisé mathématiquement le processus en 1991 \cite{ref4} et démontré certaines de ces propriétés, en particulier une propriété très utile de commutativité présentée ici partie \ref{abel}. Le modèle de croissance qu'ils proposent est alors original car il s'agit du premier modèle qui semble donner des clusters de forme sphérique.

G. Lawler, M. Bramson, M. et D. Griffeath ont pour la première fois montré en 1992 dans \cite{ref5} que la forme asymptotique de la région explorée était une boule, dans le sens suivant :
on appelle $\mathbb{B}(y,R)=\{x \in \mathbb{Z}^d, \|x-y\| <R\}$ la boule dans $\mathbb{Z}^d$ de centre $y$ et de rayon $R$, et $b(n)=\mathrm{Card}(\mathbb{B}(0,n))$. On note $A(k)$ la région explorée après avoir lancé $k$ explorateurs. Alors :
\begin{The}[Lawler, Bramson, Griffeath]
 Pour tout $\epsilon > 0$, presque sûrement,
 \begin{equation*}
  \mathbb{B}(0,n(1-\epsilon )) \subset A(b(n)) \subset \mathbb{B}(0,n(1-\epsilon ))
 \end{equation*}
 pour tout $n$ assez grand.
\end{The}

Puis, le problème de l'ordre des fluctuations autour de la boule a été étudié par G. Lawler qui montre en 1995 \cite{ref6} que les fluctuations sont presque sûrement majorées par $n^{1/3} \log ^4 (n)$ en dimension $2$ ou plus. Toutefois, des simulations suggèrent que l'ordre des fluctuations est plutôt logarithmique. Aucune amélioration notable de ces résultats n'est publiée, jusqu'en 2010, lorsque A. Asselah et A. Gaudillière montrent dans \cite{ref} que l'erreur est presque sûrement inférieure à $\alpha\sqrt{\log(n)}$ en dimension 3 et $\alpha \log(n)$ en dimension 2. Ils publient un second article \cite{ref2} auquel nous nous intéressons ici, qui établit l'optimalité de ce résultat, du moins pour ce qui est de l'erreur interne.

\newpage
\section{Premières Propriétés}
On appelle $\mathbb{B}(y,R)=\{x \in \mathbb{Z}^d, \|x-y\| <R\}$ la boule dans $\mathbb{Z}^d$ de centre $y$ et de rayon $R$. Le bord de cette boule est alors :
\begin{equation*}
 \partial \mathbb{B}(y,R)=\{x \notin \mathbb{B}(y,R) \ | \ \exists z \in \mathbb{B}(y,R), \ \| x-z\| <1\}
\end{equation*}

Commençons par définir ce qu'est la région explorée par N explorateurs :
\begin{Def}
 On appelle explorateur une marche aléatoire symétrique de $\mathbb{Z}^d$ partant d'un point $z$. Tous les explorateurs seront pris indépendants.
 Soit $\Lambda$ la région de $\mathbb{Z}^d$ préalablement explorée et $\xi^n=(\xi_1,\dots,\xi_n)$ les positions de départs des explorateurs qui suivent des marches aléatoires indépendantes $(S_1,\dots,S_n)$ symétriques sur $\mathbb{Z}^d$. 
 On définit alors récursivement $A(\Lambda,\xi^n)$ la région explorée par les explorateurs de la manière suivante :
\begin{itemize}
 \item $A(\Lambda,\emptyset)=\Lambda$
 \item $A(\Lambda,\xi^k)=A(\Lambda,\xi^{k-1})\cup\{S_k(\tau_k)\}$ 
 où $\tau_k$=$\inf \{t\geqslant0 | S_k(t)\notin A(\Lambda,\xi^{k-1})\}$ est le premier temps pour lequel $S_k$ sort de la région explorée. On dit que l'explorateur s'installe à l'instant $\tau_k$.
\end{itemize}

 On notera par la suite $A(\xi^n)=A(\emptyset,\xi^n)$ et $A_R(\xi^n)=A(\xi^n)\cap\mathbb{B}(0,R)$. On notera aussi plus simplement $A(n)=A( (0,\dots,0) )$ où le vecteur est de taille $n$.
 \end{Def}
 
 On va désormais essayer d'estimer la forme asymptotique de cette région explorée pour des explorateurs partant de l'origine, et notamment la différence entre celle-ci et une boule correspondante, c'est à dire la boule de volume le nombre d'explorateurs lancés. On va définir pour cela les notions d'erreur interne et externe.
 
 L'erreur externe, notée $\delta_O$, est la différence entre le tentacule le plus long et le rayon du cercle limite:
 \begin{equation}
  n+\delta_O(n)=\inf \{p\geqslant0\ |\ A(b(n))\subset\mathbb{B}(0,p)\}
 \end{equation}
\\où $b(n)=\mathrm{Card}(\mathbb{B}(0,n))$ est le ``volume'' de la boule de rayon $n$.
L'erreur interne, notée $\delta_I$, est la différence entre le rayon du cercle limite et le trou le plus profond:
 \begin{equation}
  n-\delta_I(n)=\sup \{p\geqslant0\ |\ \mathbb{B}(0,p)\subset A(b(n))\}
 \end{equation}
 Il est important ici de constater que ces deux notions, illustrées sur la figure suivante, ne sont définies que lorsqu'on lance un nombre d'explorateur de la forme $b(k)$.
 \begin{figure}[H]
 \centering
   \includegraphics{erreur_interne_et_externe.png}
   \caption{Exemple de région explorée par $b(13)$ explorateurs}
 \end{figure}


 Le premier résultat concernant les erreurs internes et externes assure que la région explorée ressemble bien asymptotiquement à une boule.
 \begin{The}
 \label{limsup}
  En dimension $d \geqslant 3$, $\exists\beta_d>0$ tel que
  \begin{equation}
\limsup\limits_{n\to\infty}\frac{\delta_I(n)}{\sqrt{\log(n)}}\leqslant\beta_d
\ et\ \limsup\limits_{n\to\infty}\frac{\delta_O(n)}{\sqrt{\log(n)}}\leqslant\beta_d   
  \end{equation}
  En dimension 2, $\exists\beta_2>0$ tel que
   \begin{equation}
\limsup\limits_{n\to\infty}\frac{\delta_I(n)}{\log(n)}\leqslant\beta_2
\ et\ \limsup\limits_{n\to\infty}\frac{\delta_O(n)}{\log(n)}\leqslant\beta_2  
  \end{equation}
 \end{The}
 
 Pour illustrer ce résultat, voici un exemple d'évolution de $\frac{\delta_I(n)}{\log(n)}$ et $\frac{\delta_O(n)}{\log(n)}$ en dimension 2:
 \begin{figure}[H]
  \begin{minipage}[c]{.46\linewidth}
    \includegraphics{erreurinterne.png}
    \caption{Erreur interne}
  \end{minipage}
  \hfill
 \begin{minipage}[c]{.46\linewidth}
  \includegraphics{erreurexterne.png}
  \caption{Erreur externe}
 \end{minipage}
 \end{figure}

 On peut naturellement s'interroger sur l'optimalité de cet ordre de fluctuations en $\sqrt{\log(n)}$ pour $d\geqslant3$, $\log(n)$ pour $d=2$. On va prouver ici deux théorèmes pour étudier cette question.
 \begin{The}
 \label{ext}
  En dimension $d \geqslant 3$, $\exists\alpha_d$>0 tel que
  \begin{equation}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<h(n))=1
  \end{equation}
  où $h(n)=\alpha_d\sqrt{\log(n)}$.
  
  En dimension $d=2$, $\exists\alpha_2 > 0$ tel que
  \begin{equation}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<h(n))=1
  \end{equation}
  où $h(n)=\alpha_2\sqrt{\log(n)\log(\log(n))}$
 \end{The}
Autrement dit, si les trous ne sont pas trop grand, alors de longs tentacules apparaissent. Ce théorème montre que l'erreur externe croît comme on s'y attendait sous l'hypothèse d'une erreur interne faible, mais il ne permet malheureusement pas de conclure. 
\pagebreak[1]

Le cas de l'erreur interne est quand à lui complètement déterminé en dimension supérieure à 3 grâce au résultat suivant, qui dit que presque sûrement de grands trous apparaissent une infinité de fois :
 \begin{The} \label{int}
  En dimension $d \geqslant 3$, $\exists\alpha_d>0$ tel que
  \begin{equation}
   P(\limsup\limits_{n\to\infty}\{\delta_I(n)\geqslant h(n)\})=1
  \end{equation}
  où $h(n)=\alpha_d\sqrt{\log(n)}$
 \end{The}
 
Introduisons dès à présent quelques définitions qui nous seront utiles par la suite.
\begin{Def}Soit $\Lambda$ une partie de $\mathbb{Z}^d$, S une marche aléatoire discrète, $\gamma$ et $R$ des entiers positifs.
\begin{enumerate}[i)]
\item $\rho(\gamma)=\sup\{n\geqslant 0,\ b(n)\leqslant \gamma\}$ est le rayon de la plus grande boule de volume inférieur à $\gamma$.
  
  \item $H(S;\Lambda)=\inf\{n\geqslant0,\ S(n)\in\Lambda\}$ (ou $H(\Lambda)$ en l'absence d'ambiguïté sur $S$) est le premier instant pour lequel la marche aléatoire $S$ atteint $\Lambda$.
  
  \item Si $\eta:\mathbb{Z}^d\to\mathbb{N}$ est telle que $\eta(z)$ est le nombre d'explorateurs partis de $z$, la région explorée en partant de $\eta$ sera la région définie en faisant partir les explorateurs dans un ordre prédéfini sur $\mathbb{Z}^d$. 
  
  \item $W_R(\Lambda;\eta, z)$ est le nombre d'explorateurs, partant de la configuration $\eta$ et de la région explorée $\Lambda$, qui passent par $z\in\mathbb{B}(0,R)$ avant de sortir de $\mathbb{B}(0,R)$.
 
  \item $W_R(\Lambda;\eta, z)$ est le nombre d'explorateurs, partant de la configuration $\eta$ et de la région explorée $\Lambda$, qui passent par $z\in\partial\mathbb{B}(0,R)$ au moment où ils sortent de $\mathbb{B}(0,R)$.

  \item Pour $z \in \mathbb{Z}^d \setminus \{ 0 \}$ on note $\Sigma (z) = \partial \mathbb{B} (0, \| z \| )$. Remarquons que $z \in \Sigma (z)$.
  
  \item $\zeta_z^N$ est le vecteur $(z,\dots,z)$ de taille $N$.
\end{enumerate}
 \end{Def}
 \begin{Rq}
  Si $\mathbb{B}(0,R)\subset\Lambda$, $W_R(\mathbb{B}(0,R);\eta,z)=W_R(\Lambda;\eta,z)$. On appelle alors cette quantité $M_R(\eta,z)$.
 \end{Rq}

 \newpage
 \section{Propriété abélienne}
 \label{abel}
 Insérer ici la propriété abélienne si on fait la démo avec l'exemple qui sert après. Dire que ça justifie l'utilisation d'un ordre choisi au pif sur Zd
 \\Mémo : Je me sers dans le dernier lemme de l'erreur externe du résultat qui est sur le livre qu'on a emprunté. Si on l'a démontré, penser à modifier ça.
 \\Dans le même lemme je me sers aussi du fait que b(n+h(n))-b(n)$\leqslant$ch(n)$n^{d-1}$ et la même chose dans l'autre sens. Le démontrer un jour.
\begin{figure}[H]
 \centering
   \includegraphics{vague.jpg}
   \caption{La région explorée par vagues}
\end{figure}
 \newpage
 
 \section{L'erreur externe}
 On va prouver ici le théorème \ref{ext}, d'abord en dimension supérieure à 3, puis en dimension 2.
 \subsection{En dimension supérieure à 3}
 Pour $\alpha$ et $\gamma$ choisis par la suite, on pose $h(n)=\alpha\sqrt{\log(n)}$ et $L(n)=\gamma\sqrt{\log(n)}$. $h$ représente ici l'erreur externe et $L$ l'erreur interne, et il est important de conserver cette distinction à l'esprit même si on prendra $\alpha=\gamma$ par la suite.
 On rappelle qu'on veut démontrer ici la formule suivante:
 \begin{equation*}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<L(n))=1
\end{equation*}
Pour estimer l'erreur externe, on va commencer par minorer la probabilité de fabriquer un tentacule qui sorte de $\mathbb{B}(0,R)$ lorsqu'on fait  partir $N$ explorateurs de $z$, avec $\|z\|<R$ (on prendra par la suite $R$ de la forme $\|z\|+h(n)$).
\begin{Prop}
\label{step2}
 Il existe $c>1$ tel que $\forall z\in\mathbb{Z}^d$, si on prend $R$ tel que $N\geqslant c(R-\|z\|)$ et $R\geqslant\|z\|+1$, on ait, pour tout $\Lambda\subset\mathbb{Z}^d$,
 \begin{equation*}
  P(A(\Lambda,\zeta_z^N)\not\subset\mathbb{B}(0,R))\geqslant \exp (-c(R-\|z\|)^2)
 \end{equation*}
\begin{proof}
 On commence par remarquer qu'on peut prendre $\Lambda$=$\emptyset$ car \newline $A(\emptyset,\xi)\subset A(\Lambda,\xi) \Rightarrow P(A(\Lambda,\xi)\not\subset\mathbb{B}(0,R))\geqslant P(A(\emptyset,\xi)\not\subset\mathbb{B}(0,R))$.
 Le reste de la démonstration consiste à créer un tentacule qui sort de $\mathbb{B}(0,R)$ en partant de $z$. Pour cela posons 
 \begin{equation*}
  x_n=(\|z\|+n)\frac{z}{\|z\|}
 \end{equation*}
La suite $x_n$ est une suite de points partant de $z$ et qui arrive sur $\mathbb{B}(0,R)$ perpendiculairement à celle-ci. Pour tout cube unitaire centré en $x_n$, il existe $z_n\in\mathbb{Z}^d$ dans ce cube. Soit $p$ le plus petit entier tel que $\|x_p\|\geqslant R+1$, alors $\|z_p\|\geqslant R$. 
On définit alors $(y_1=z,y_2,\dots,y_k=z_p)$ une suite de plus proches voisins dans $\mathbb{Z}^d$, où l'on va chercher à prendre $k$ suffisamment petit. 

On peut trouver une constante $c_1$ qui permet de prendre $k\leqslant c_1p$. Pour cela, on procède de la façon suivante: on appelle $C$ le diamètre de la figure composée de deux cube unitaire partageant une face, de façon à avoir $\|z_n-z_{n-1}\|\leqslant C$. Il est alors possible de relier $z_n$ à $z_{n-1}$ par une suite de plus proches voisins de taille inférieure à $c_1=dC$, ce qui prouve le résulat.
On alors $k\leqslant c_1(R-\|z\|+2) \leqslant c(R- \|z\|)$ pour $c$ une constante suffisamment grande. Comme $N \geqslant c(R-\|z\|)$, on peut forcer les $k$ premiers explorateurs à s'installer successivement sur $y_1, \dots, y_k$, et la probabilité de cet événement est :
\begin{equation*}
 (\frac{1}{2d})^{\sum_{n=1}^{k}{n}}\geqslant \exp(-c(R-\|z\|)^2)
\end{equation*}
quitte à augmenter la constante $c$. La présence d'un tel tentacule impliquant que les explorateurs sortent de $\mathbb{B}(0,R)$, on a bien le résultat voulu.
\end{proof}
 \end{Prop}
 On fixe désormais l'entier $n$, et on va essayer d'estimer la probabilité de trouver un entier $k\geqslant n$ pour lequel l'erreur externe dépasse $h(k)$. Pour cela on va agir comme décrit dans la section \ref{abel} en lançant les explorateurs de l'origine en deux vagues, la première de $b(n)$ explorateurs et la deuxième de $X_n$ explorateurs, où $X_n$ est une variable aléatoire discrète indépendante des marches des explorateurs, dont la loi sera fixée par la suite. On arrête ces derniers explorateurs sur $\Sigma=\partial\mathbb{B}(0,r_n)$ avant de les faire repartir, avec $r_n = n-L(n)$. 
 
 La propriété abélienne montre alors que la région explorée de cette façon est la même, en loi, que celle explorée en lançant directement $X_n+b(n)$ explorateurs. On cherche à trouver $X_n$ tel que l'évènement $\delta_O(R_n)\geqslant h(R_n)$ où $R_n=\rho(b(n)+X_n)+1$ arrive avec une probabilité proche de $1$.

\medskip
Après avoir arrêté la deuxième vague sur $\Sigma$, on considère l'événement : partant d'un point $z$ de $\Sigma$, en relançant les explorateurs arrêtés en $z$, on construit un tentacule qui sort de $\mathbb{B}(0,R_n+\delta_O(R_n))$. On note cet évènement $\mathrm{cov}(z)$ :
\begin{equation}
 \mathrm{cov}(z)=\{A(\Lambda_n,\zeta_z^{N_z})\not\subset\mathbb{B}(0,R_n+h(R_n))\}
\end{equation}
où $N_z$ est le nombre d'explorateurs arrétés en $z$ et $\Lambda_n$ est la région explorée au moment où on a arrété la deuxième vague.
\begin{figure}[H]
 \centering
   \includegraphics{cov.png}
   \caption{Un exemple où $\mathrm{cov}(z)$ est réalisé}
\end{figure}
On constate que l'évènement $\delta_O(R_n)\geqslant h(R_n)$ est lié à $\{\mathrm{cov}(z), z\in\Sigma\}$ de la façon suivante :
\begin{align}
\nonumber P(\delta_O(R_n)\geqslant h(R_n))&\geqslant P(A(b(R_n))\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(A(b(n)+X_n)\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(A(\Lambda_n,\sum_{z\in\Sigma}\zeta_z^{N_z})\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov}(z))
\end{align}

Il nous faut désormais estimer $P(\bigcup_{z\in\Sigma}\mathrm{cov}(z))$. Pour cela, il serait intéressant d'avoir indépendance entre les $\mathrm{cov}(z)$ sous certaines hypothèses, ce qu'on va obtenir en prenant pour $X_n$ une variable de Poisson. En effet, les variables de Poisson ont la propriété de découpage \ref{indé}, qui laisse penser que les $N_z$ seront indépendantes et de loi connue.

Cette propriété ne permet pas pour autant d'affirmer que les $N_z$ sont des variables de Poisson pour $z \in \Sigma$, mais cela sera vrai lorsque $\delta_I(n) < L(n)$. On définit donc pour tout $z\in\Sigma$
\begin{equation}
 \mathrm{cov_2}(z)=\{A(\Delta_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,R_n+h(R_n))\}
\end{equation}
où $N'_z$ est le nombre d'explorateurs de la première vague qui touchent $\Sigma$ en $z$ lorsque la région explorée initiale est $\Delta_n=\mathbb{B}(0,r_n)$.
\begin{figure}[H]
 \centering
   \includegraphics{cov2.png}
   \caption{Un exemple où $\mathrm{cov_2}(z)$ est réalisé}
\end{figure}
On remarque que la seule différence entre $\mathrm{cov}(z)$ et $\mathrm{cov_2}(z)$ est la région explorée avant de lancer la deuxième vague. C'est ici qu'il est important de se placer dans le cas $\delta_I(n)<L(n)$, et on restera dans ce cas jusqu'à la fin de cette partie, puisqu'on a alors:
\begin{equation*}
 W_{n-L(n)}(A(\zeta_0^{b(n)});X_n1_0,z)=M_{n-L(n)}(X_n1_0,z)
\end{equation*}
et donc $N'_z=N_z$ et $\mathrm{cov_2}(z) \subset \mathrm{cov}(z)$. 
\pagebreak[1]

Soit $U$ la variable aléatoire à valeur dans $\Sigma$ calculée en regardant en quel point une marche aléatoire symétrique atteint $\Sigma$. On alors :
\begin{equation*}
 N'_z=\sum_{p\leqslant X_n}{1_{U_p=z}}
\end{equation*}
D'après la proposition \ref{indé} les $N'_z$ sont alors des variables de Poisson indépendantes, et donc les $\mathrm{cov_2}(z)$ sont indépendants. Ils sont de plus indépendants de $\delta_I(n)$ puisque celui-ci ne dépend que de la première vague alors que $\mathrm{cov_2}$ en est indépendant. Résumons ce qu'on a fait jusqu'à présent :

\begin{align}
\label{16}
\nonumber P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<L(n))&\geqslant P(\delta_0(R_n)\geqslant h(R_n)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov}(z)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&=P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z))
\end{align}

Il faut maintenant trouver $\lambda_n$, le paramètre de la loi de Poisson $X_n$, pour que tout ceci tende vers $1$.
La technique consiste à majorer $R_n+h(R_n)$ de façon à pouvoir appliquer la proposition \ref{step2}:
\begin{Lemme}
\label{pois}
Pour $n$ suffisamment grand,
 \begin{equation}
  X_n\leqslant b(n+h(n))-b(n)\Rightarrow R_n+h(R_n)\leqslant n+3h(n) 
 \end{equation}
 \begin{proof}
  \begin{align*}
   X_n\leqslant b(n+h(n))-b(n)\Rightarrow X_n+b(n)\leqslant b(n+h(n))&\Rightarrow R_n-1\leqslant n+h(n)
   \\&\Rightarrow h(R_n)\leqslant 2h(n)-1
  \end{align*}
  pour $n$ suffisamment grand, d'où le résultat.
 \end{proof}
\end{Lemme}

On prend alors $\lambda_n=\frac{b(n+h(n))-b(n)}{2}$ pour pouvoir se placer dans le cas du lemme précédent sans difficulté, en effet, on aura souvent $X_n$ plus petit que $2\lambda_n$ : d'après la propriété annexe \ref{deuxmoy}, 
\begin{equation}
\lim_{n \to \infty} P(X_n) < 2\lambda_n = 1
\end{equation}

En utilisant le lemme \ref{pois}, on a donc :
\begin{align}
\label{17}
\nonumber P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z))&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z)\cap\{X_n\leqslant\lambda_n\})
\nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\{A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n))\})P(X_n\leqslant\lambda_n)
\nonumber\\&=P(X_n\leqslant\lambda_n)\big(1-\prod_{z\in\Sigma}\big(1-P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n)))\big)\big)
\end{align}
Or, d'après la proposition \ref{step2},
\begin{align}
\label{18}
\nonumber &P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n)))
\nonumber\\&=P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n))\ \normalfont{|}\ N'_z\geqslant c(3h(n)+L(n)))P(N'_z\geqslant c(3h(n)+L(n)))
\nonumber\\&\geqslant \exp(-c(3h(n)+L(n))^2)P(N'_z\geqslant c(3h(n)+L(n)))
\end{align}

Il ne reste donc plus qu'à minorer $P(N'_z\geqslant c(3h(n)+L(n)))$:
\begin{Lemme}
 $\forall c_1>0, \exists C\geqslant 0$ tel que
 \begin{equation}
 \label{11}
  P(N'_z\geqslant c_1h(n))\geqslant \exp (-Ch(n))
 \end{equation}
 \begin{proof}
  D'après la proposition \ref{indé}, $N'_z$ est une variable de poisson de paramètre $\mu=\lambda_n P(S(H(\Sigma))=z)$ où S est une marche aléatoire symétrique sur $\mathbb{Z}^d$ partant de $0$. En utilisant le lemme 1.7.4 de \cite{ref3}, on a des constantes $c_2$ et $c_3$ telles que :
  \begin{equation*}
   \frac{c_2}{(n-L(n)+1)^{d-1}}\leqslant\frac{c_2}{\|z\|^{d-1}}\leqslant P(S(H(\Sigma))=z)\leqslant\frac{c_3}{\|z\|^{d-1}}\leqslant\frac{c_3}{(n-L(n))^{d-1}}
  \end{equation*}
 De plus, d'après le lemme \ref{b(n)}, $c_4h(n)n^{d-1}\leqslant\lambda_n\leqslant c_5h(n)n^{d-1}$, donc $c_6 h(n) \leqslant\mu\leqslant c_7 h(n)$. On a donc, en posant $a=\lfloor c_1 h(n) \rfloor +1$ :
 \begin{align*}
  P(N'_z\geqslant c_1h(n))&\geqslant P(N'_z=a)=e^{-\mu}\frac{\mu^a}{a!}
  \\&\geqslant e^{-c_7h(n)}\frac{(c_6h(n))^{c_1h(n)}}{a!}
  \\&\sim e^{-c_7h(n)}\frac{(c_6h(n))^{c_1h(n)}}{\sqrt{2\pi a}}\Big(\frac{e}{c1h(n)}\Big)^{[c_1h(n)]+1}
  \\&\geqslant  e^{(-c_7+c_1(1-ln(c_1)+ln(c_6)))h(n)+2}\frac{1}{h(n)^2\sqrt{2\pi a}}
  \\&\geqslant \exp(-Ch(n))
 \end{align*}
 \end{proof}
\end{Lemme}

En prenant $h(n)=L(n)$ et en utilisant \ref{16}, \ref{17}, \ref{18}, \ref{11} et que $1-x\leqslant e^{-x}$, on a finalement :
\begin{align}
 \nonumber&P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\normalfont\ {|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)\big(1-\prod_{z\in\Sigma}\big(1-\exp(-c(4h(n))^2-Ch(n))\big)\big)
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1}\exp(-Ch(n)-c'h(n)^2)))
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1}\exp(-c''h(n)^2)))
 \nonumber\\&=P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1-\alpha^2c''}))
\end{align}

Il suffit maintenant de fixer $\alpha$ suffisamment petit pour que $d-1-\alpha^2 c''$ soit positif pour terminer la preuve du théorème \ref{ext}.
\medskip
\pagebreak[1] %n'efface pas cette commande elle permet de «suggérer» à Latex de sauter une page ici, sans l'obliger, pour avoir un meilleur rendu.

Il n'est pas clair \emph{a priori} que ce théorème nous donne vraiment une indication sur l'optimalité de $h(n)$ pour l'erreur externe, mais on peut voir qu'il implique le résultat intéressant suivant :
\begin{Cor}
 \begin{equation} \label{io}
   P(\limsup\limits_{n\to\infty}\{\delta_I(n)\geqslant L(n)\}\cup\{\delta_O(n)\geqslant h(n)\})=1
  \end{equation}
\begin{proof}
On a :
 \begin{align*}
   &P(\exists k\geqslant n,\delta_I(k)\geqslant L(k)\ \normalfont{ou}\ \delta_O(k)\geqslant h(k))
   \\&\geqslant P(\delta_I(n)\geqslant L(n)\ \normalfont{ou}\ \exists k\geqslant n,\delta_0(k)\geqslant h(k))
   \\&= P(\delta_I(n)\geqslant L(n))+P(\exists k\geqslant n,\delta_0(k)\geqslant h(k)\normalfont\ {|}\ \delta_I(n)<L(n))P(\delta_I(n)<L(n))
   \\&= 1-P(\delta_I(n)<L(n))\big(1-P(\exists k\geqslant n,\delta_0(k)\geqslant h(k)\ \normalfont{|}\delta_I(n)<L(n))\big)
 \end{align*}
Et donc,
\begin{align*}
 &P(\limsup\limits_{n\to\infty}\{\delta_I(n)\geqslant L(n)\}\cup\{\delta_O(n)\geqslant h(n)\})\\&=\lim_{n\to\infty}P(\exists k\geqslant n,\delta_I(k)\geqslant L(k)\ \normalfont{ou}\ \delta_O(k)\geqslant h(k))\\&=1
\end{align*}
\end{proof}

\end{Cor}
Autrement dit, pour une infinité de termes, l'erreur interne ou l'erreur externe est d'ordre $\sqrt{\log n}$, donc le théorème \ref{limsup} est d'une certaine façon optimal. On verra dans la partie \ref{interne} que \ref{io} est en fait vraie pour l'erreur interne seule.

\subsection{En dimension 2}


\section{L'erreur interne}
\label{interne}
Pour établir le Théorème \ref{int}, nous allons procéder de façon similaire. Nous lançons cette fois-ci non pas une vague d'explorateurs à partir de $A(b(n))$, mais un grand nombre $F(n)$ de vagues successives. Pour $k \leqslant F(n)$, la $k^{\mathrm{\grave{e}me}}$ vague consistera en un lancer de $X_k$ explorateurs depuis l'origine, où $X_k$ sera une varible de Poisson de paramètre $\lambda_k$ à déterminer plus tard.

On note $R_k$ le rayon théorique après la $k^{\mathrm{\grave{e}me}}$ vague :
\begin{equation}
 R_k = \rho (b(n) + X_1 + \dots + X_k)
\end{equation}

Et $A_k$ l'évènement qui nous intéresse :
\begin{equation}
 A_k = \{ \delta_I (R_k) > \sqrt{\log (R_k) } \} 
\end{equation}

Le but sera de montrer que 
\begin{equation}
 \lim _{n \to \infty } P \left( \bigcup _{k \leqslant F(n)} A_k \right) = 1
\end{equation}

Une propriété simple nous sera utile par la suite :
\begin{Rq}
 Quel que soit $k \leqslant F(n)$, on a toujours :
 \begin{equation} \label{nonvide}
  A(b(R_k))^c \cap (\mathbb{B} (0, R_k)\cup\partial\mathbb{B} (0, R_k)) \neq \emptyset
 \end{equation}
 En effet, le cas échéant, on aurait strictement plus de $b(R_k)$ sites occupés, ce qui est impossible.
\end{Rq}

\subsection{Lorsque $A^c_{k-1}$ est réalisé}
On suppose dans cette partie que l'évènement $A^c_{k-1}$ est réalisé, pour $1 \leqslant k \leqslant F(n)-1$. Utilisons alors l'ensemble non vide donné par \ref{nonvide} au rang $k-1$ : il existe un $Z_k$ non encore visité, tel que $\|Z_k\| \leqslant R_{k-1}$. Comme $A^c_{k-1}$ est réalisé, nécessairement, 
\begin{equation}
 R_{k-1} - h(R_{k-1}) \leqslant \|Z_k\| \leqslant R_{k-1}.
\end{equation}
On va s'appuyer sur ce $Z_k$ pour construire un évènement $I_k$ qui pourra s'interpréter par : soit $Z_k$ est un trou laissé à la prochaine vague $k$, soit un long tentacule apparaît à la vague $k$.

\begin{Def}
\begin{enumerate}[i)]
 \item
Pour $\Lambda \subset \Sigma (z)$, $n$ un entier et $R > 0$, on définit la variable aléatoire $N_z (n, \Lambda, R)$ comme le nombre de marches aléatoires parmi $n$ marches partant de $0$ qui atteignent $\Sigma (z)$ en $\Lambda$ puis visitent $z$ avant de sortir de $\mathbb{B} (0, \| z \| + R)$ :
\begin{equation}
 N_z (n, \Lambda, R) = \mathrm{Card} \{ i \leqslant n \ \mid \ S_i (H(\Sigma (z))) \in \Lambda, \ H(z) < H( \mathbb{B}^c (O, \| z \| + R )) \}
\end{equation}
 \item Soit $\Lambda_k = \mathbb{B}(Z_k, L(R_k)) \cap \Sigma (Z_k)$ et $\Lambda'_k = \Sigma (Z_k) \setminus \Lambda_k$ et :
 \begin{equation}
  I_k = \{ N_{Z_k} (X_k, \Lambda_k, \infty ) = 0, N_{Z_k} (X_k, \Lambda '_k, L(R_{k-1})) = 0 \}
 \end{equation}
\end{enumerate}
\end{Def}

Si $I_k$ est réalisé, on aimerait pouvoir dire que dans le cas où $Z_k$ n'est pas visité (un trou apparaît) on a $\delta_I (R_k) \geq h(R_k)$ et que dans le cas contraire (un long tentacule apparaît) on a $\delta_O (R_k) \geq L(R_k)$. Cela sera assuré pour $X_k$ dans un bon intervalle, donné par le lemme suivant, qui induira du coup le choix des paramètres $\lambda_k$ :
\begin{Lemme}
Pour $k$ suffisamment grand, si $\mu_k = b(R_{k-1} + h(R_{k-1})) - b(R_{k-1})$,
\begin{equation} \frac{2}{3} \mu_k \leq X_k \leq 2 \mu_k \Rightarrow I_k \subset A_k \cup \{ \delta_O (R_k) \geq L(R_k) \}
\end{equation}
\end{Lemme}
\begin{proof}
plus tard...
\end{proof}
On voit donc qu'il est judicieux de prendre $\lambda_k = \mu_k = b(R_{k-1} + h(R_{k-1})) - b(R_{k-1})$. Ainsi, lorsque la variable de Poisson $X_k$ est comprise entre $\frac{2}{3}$ et $2$ fois sa moyenne, on a bien $I_k \subset A_k \cup \{ \delta_O (R_k) \geq L(R_k) \}$.

Enfin, on a besoin de s'assurer que la probabilité de $I_k$ est assez grande, ce qui est donné par le lemme technique suivant :

\begin{Lemme}
 Il existe une constante $C > 0$ telle que, si $X$ une variable de Poisson de paramètre $\lambda$, $z$ un point de $\in \mathbb{Z}^d \setminus \{ 0 \}$ et $R > 0$, 
 \begin{equation*}
  P(N_z (X, \Lambda, \infty ) = 0, N_z (X, \Lambda ', R) = 0) \geqslant \exp \left( - C \frac{\lambda R}{\| z \| ^{d-1} } \right)
 \end{equation*}
 où $\Lambda = \mathbb{B} (z, R) \cap \Sigma (z)$ et $\Lambda ' = \Sigma (z) \setminus \Lambda$.
\end{Lemme}
\begin{proof}
 D'après la proposition \ref{indé}, $N_z (X, \Lambda, \infty )$ et $N_z (X, \Lambda ', R)$ sont des variables de Poisson indépendantes. Ainsi :
\begin{equation*} P(N_z (X, \Lambda, \infty ) = 0, N_z (X, \Lambda ', R) = 0) = \exp \left( - E[N_z (X, \Lambda, \infty )] - E[N_z (X, \Lambda ', R)] \right)
\end{equation*}

On va montrer que chacune de ces espérances est majorée par un terme du type $c \frac{\lambda R}{\| z \| ^{d-1} }$.
\begin{itemize}
 \item Pour la première, on se sert du fait que $P_0 (S(H(\Sigma (z))) = y) \leqslant \frac{c_1} {\| z \| ^ {d-1}} $ (Lemme 1.7.4 de \cite{ref3}) et $P_y (H(z) < \infty ) \leqslant \frac{c_2}{1+\| y-z \| ^{d-2}}$ (Théorème 1.5.4 de \cite{ref3}) pour écrire :
 \begin{eqnarray}
 E[N_z (X, \Lambda, \infty )] & = & \lambda \ P_0 (S(H(\Sigma (z))) \in \Lambda, \ H(z) < \infty)
 \nonumber\\& = & \lambda \sum _{y \in \Lambda} P_0 (S(H(\Sigma (z))) = y) \ P_y (H(z) < \infty )
 \nonumber\\& \leqslant & \lambda \sum _{y \in \Lambda} \frac{c_1} {\| z \| ^ {d-1}} \ \frac{c_2}{1+\| y-z \| ^{d-2}}
 \end{eqnarray}
 En arrangeant plutôt la somme selon $k = \lfloor \| y-z \| \rfloor$, et en observant que pour un $k$ fixé on a moins de $c_3 k^{d-2}$ positions de $y$ correspondants, on obtient :
 \begin{eqnarray}
 E[N_z (X, \Lambda, \infty )] & \leqslant & c_4 \frac{\lambda} {\| z \| ^ {d-1}} \left( 1 + \sum _{k=1} ^{R} \frac{k^{d-2}}{1+k^{d-2}} \right)
 \nonumber\\& \leqslant & c_5 \frac{\lambda R} {\| z \| ^ {d-1}}
 \end{eqnarray}
 
 \item Pour la seconde espérance, on utilise le résultat du Lemme 5(b) de \cite{ref7} qui assure que $P_y ( H(z) < H( \mathbb{B}^c (O, \| z \| + h )) ) \leqslant \frac{c_6 R^2}{\| z-y \| ^d}$, et on utilise la même transformation sur la somme :
 \begin{eqnarray}
 E[N_z (X, \Lambda ', R)] & = & \lambda \ P_0 (S(H(\Sigma (z))) \in \Lambda ', \ H(z) < H( \mathbb{B}^c (O, \| z \| + h )) )
 \nonumber\\& = & \lambda \sum _{y \in \Lambda '} P_0 (S(H(\Sigma (z))) = y) \ P_y (H(z) < H( \mathbb{B}^c (O, \| z \| + h )) )
 \nonumber\\& \leqslant & \lambda \sum _{y \in \Lambda '} \frac{c_1} {\| z \| ^ {d-1}} \ \frac{c_6 R^2}{\| z-y \| ^d}
 \nonumber\\& \leqslant & c_7 \frac{\lambda R^2} {\| z \| ^ {d-1}} \sum _{k=R} ^{2\|z\|+2} \frac{k^{d-2}}{k^{d}}
 \nonumber\\& \leqslant & c_8 \frac{\lambda R} {\| z \| ^ {d-1}}
 \end{eqnarray}
\end{itemize}
\end{proof}
En appliquant ce lemme à $X_k$, $Z_k$ et $L(R_{k-1})$, on obtient :
\begin{eqnarray}
 P(I_k) & \geq & \exp \left( - C \frac{\lambda_k L(R_{k-1}) }{\| Z_k \| ^{d-1} } \right)
 \nonumber\\& \geq & \exp \left( - C \frac{\lambda_k L(R_{k-1}) }{(R_{k-1} - h(R_{k-1}))^{d-1} } \right)
 \nonumber\\& \geq & \exp \left( - \kappa h(R_{k-1}) L(R_{k-1}) \right)
\end{eqnarray}
car $\lambda_k$ est de l'ordre de $R_{k-1}^{d-1}h(R_{k-1})$ d'après le lemme \ref{b(n)} et $R_{k-1} - h(R_{k-1})$ est de l'ordre de $R_{k-1}$
\subsection{Minorations}
Résumons ce que l'on a fait jusqu'à présent : on a montré que lorsque $A^c_{k-1}$ est réalisé, il existe un évènement $I_k$ tel que $\frac{2}{3} \lambda_k \leq X_k \leq 2 \lambda_k \Rightarrow I_k \subset A_k \cup \{ \delta_O (R_k) \geq L(R_k) \}$ et $P(I_k) \geqslant \exp 
\left( - \kappa h(R_{k-1}) L(R_{k-1} \right)$. Notre but est de montrer que $\lim_{n \to \infty } P \left( \bigcup _{k \leqslant F(n)} A_k \right) = 1$.


 
\newpage

\appendix

\section{Propriétés annexes}
\subsection{Découpage d'une variable de Poisson}
\begin{Prop}
\label{indé}
Si $\{U_n,n\in\mathbb{N}\}$ est une suite i.i.d. de variables aléatoires de loi $U$ à valeur dans $E$, $X$ une variable de Poisson indépendante des $U_n$ de paramètre $\lambda$, $\{E_i,i\in\{1,\dots,n\}\}$ une partition de E, et si on pose 
\begin{equation*}
 X_i=\sum_{n\leqslant X}{1_{U_n\in E_i}}
\end{equation*}
alors les $X_i$ sont des variables de Poisson indépendantes de paramètre $\lambda P(U \in E_i )$
\end{Prop}
\label{demindé}
\begin{proof}\ 
Ce sont des variables de Poisson :
\begin{align*}
\mathbb{P}(X_i=k)&=\mathbb{P}(\sum_{n\leq X}{1_{U_n\in E_i}=k})
\\&=\sum_{p=k}^{\infty}{\mathbb{P}(1_{X=p}\sum_{n\leqslant p}{1_{U_n \in E_i}=k}})
\\&=\sum_{p=k}^{\infty}{\binom{p}{k}\mathbb{P}(X=p)\mathbb{P}(U_1\in E_i)...\mathbb{P}(U_k\in E_i)\mathbb{P}(U_{k+1}\notin E_i)...\mathbb{P}(U_{p}\notin E_i)}
\\&=\frac{e^{-\lambda}}{k!}\mathbb{P}(U\in E_i)^k\sum_{p=k}^{\infty}{\frac{(1-\mathbb{P}(U\in E_i))^{p-k}\lambda^{p}}{(p-k)!}}
\\&=\frac{(\lambda\mathbb{P}(U\in E_i))^k}{k!}e^{-\lambda\mathbb{P}(U\in E_i)}
\end{align*}

Elles sont indépendantes :
\begin{align*}
&\mathbb{P}(\bigcap_{i\in I}X_i=k_i)
\\&=\mathbb{P}(\sum_{n\leq X}{\bigcap_{i\in I}1_{U_n\in E_i}}=k_i)
\\&=\sum_{p=\sum_{i\in I}k_i}^{\infty}{\prod_{i\in I}\binom{p-\sum_{j\in I,j<i}k_j}{k_i}\mathbb{P}(X=p)\big(\prod_{i\in I}\mathbb{P}(U\in E_i)^{k_i}\big)\mathbb{P}(U\notin\bigcup_{i\in I} E_i)^{p-\sum_{i\in I}k_i}}
\\&=\frac{e^{-\lambda}}{\prod_{i\in I}(k_i!)}\big(\prod_{i\in I}\mathbb{P}(U\in E_i)^{k_i}\big)\sum_{p=\sum_{i\in I}k_i}^{\infty}{\frac{\lambda^p}{(p-\sum_{i\in I}k_i)!}\mathbb{P}(U\notin\bigcup_{i\in I} E_i)^{p-\sum_{i\in I}k_i}}
\\&=\prod_{i\in I}\mathbb{P}(X_i=k_i)
\end{align*}
\end{proof}

\subsection{Estimations sur une variable de Poisson}
\label{demdeuxmoy}
\begin{Prop}
 \label{deuxmoy}
 Soit $X_\lambda$ une variable de Poisson de paramètre $\lambda$. On a :
 \begin{equation*}
  \lim_{\lambda\to\infty}P(X_\lambda>2\lambda)=0
 \end{equation*}
\end{Prop}
\begin{proof}
Posons $p=2 \lfloor \lambda \rfloor +1$ :
 \begin{align*}
  P(X_\lambda>2\lambda)=e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^{k+p}}{(k+p)!}
  \leqslant e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^{k+2\lambda+2}}{(k+2\lambda)!}
  &\leqslant e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^{k+2\lambda+2}}{(2\lambda)^{k-1}(2\lambda)!}
  \\&=e^{-\lambda}\frac{\lambda^{2\lambda+3}}{(2\lambda)!}
  \\&\sim e^{-\lambda}\lambda^{2\lambda+3}\Big(\frac{e}{2\lambda}\Big)^{2\lambda}\frac{1}{\sqrt{4\pi\lambda}}
  \\&\sim \Big(\frac{e}{4}\Big)^\lambda\frac{\lambda^3}{\sqrt{4\pi\lambda}}
 \end{align*}
\end{proof}

\subsection{Estimation de $b(n)$}
\begin{Lemme}
\label{b(n)}
\begin{enumerate}[i)]
\item Soit $\omega_d$ le volume de la boule unité de $\mathbb{Z}^d$, on a :
 \begin{equation*}
  b(n)\sim\omega_dn^d
 \end{equation*}
\item Soit $a_n$ une suite telle que $a_n\to\infty$ et $a_n=o(n)$, on a :
 \begin{equation*}
   b(n+a_n)-b(n)\sim\omega_da_nn^{d-1}
 \end{equation*}
\end{enumerate}
 \begin{proof}
 \begin{enumerate}[i)]
 \item On note $\lambda$ la mesure de Lebesgue dans $\mathbb{R}^d$. Notons $V(n)=\omega_dn^d$ le volume de la boule de rayon $n$ et $\alpha_d=\sqrt d$ le diamètre d'un cube unité. On peut paver $\mathbb{R}^d$ par des cubes unités dont les centres appartiennent à $\mathbb{Z}^d$. On note $c_z$ un tel cube, avec $z$ son centre. On pose alors :
 \begin{equation*}
  A(n)=\bigcup_{c_z\cap B(0,n)\neq\emptyset}{c_z}
 \end{equation*}
On remarque que $c(n):=\lambda (A(n))= \mathrm{Card} \{z \ \mid \ c_z\cap B(0,n)\neq\emptyset\}$ vérifie :
\begin{align*}
 | c(n)-V(n) | = c(n)-V(n)&\leqslant\lambda(\bigcup_{z \ \mid \ c_z\cap B(0,n)\neq\emptyset,c_z\not\subset B(0,n)}{c_z})
 \\&\leqslant\lambda(B(0,n+\alpha_d))-\lambda(B(0,n-\alpha_d))
 \\&=((n+\alpha_d)^d-(n-\alpha_d)^d)\omega_d
 \\&\sim2\alpha_dn^{d-1}\omega_d
\end{align*}
De même :
\begin{align*}
 | c(n)-b(n) |=c(n)-b(n)&\leqslant \mathrm{Card}(\{z \ \mid \ c_z\cap B(0,n)\neq\emptyset,c_z\not\subset B(0,n)\})
 \\&\sim2\alpha_dn^{d-1}\omega_d
\end{align*}
Et on a donc :
\begin{equation*}
 \normalfont{|}\frac{b(n)}{V(n)}-1\normalfont{|}\leqslant\frac{1}{V(n)}(\normalfont{|}c(n)-b(n)\normalfont{|}+\normalfont{|}c(n)-V(n)\normalfont{|})=O(\frac{1}{n})
\end{equation*}

\item Posons :
 \begin{equation*}
  A'(n)=\bigcup_{c_z\subset B(0,n)}{c_z}
 \end{equation*}
 et $c'(n)=\lambda (A'(n))$
 \begin{align*}
  &b(n+a_n)-b(n)
  \\&=(b(n+a_n)-c(n+a_n))+(c(n+a_n)-c'(n))+(c'(n)-b(n))
  \\&\leqslant c(n+a_n)-c'(n)
  \\&\leqslant (c(n+a_n)-V(n+a_n+\alpha_d))+(V(n+a_n+\alpha_d)-V(n-\alpha_d))+(V(n-\alpha_d)-c'(n))
  \\&\leqslant V(n+a_n+\alpha_d)-V(n-\alpha_d)
  \\&=\omega_d((n+a_n+\alpha_d)^d-(n-\alpha_d)^d)
  \\&=\omega_da_nn^{d-1}+o(\omega_da_nn^{d-1})
 \end{align*}
De même :
 \begin{align*}
  &b(n+a_n)-b(n)
  \\&=(b(n+a_n)-c'(n+a_n))+(c'(n+a_n)-c(n))+(c(n)-b(n))
  \\&\geqslant c'(n+a_n)-c(n)
  \\&\geqslant (c'(n+a_n)-V(n+a_n-\alpha_d))+(V(n+a_n-\alpha_d)-V(n+\alpha_d))+(V(n+\alpha_d)-c(n))
  \\&\geqslant V(n+a_n-\alpha_d)-V(n+\alpha_d)
  \\&=\omega_d((n+a_n-\alpha_d)^d-(n+\alpha_d)^d)
  \\&=\omega_da_nn^{d-1}+o(\omega_da_nn^{d-1})
 \end{align*}
 D'où
 \begin{equation*}
   b(n+a_n)-b(n)\sim\omega_da_nn^{d-1}
 \end{equation*}
 \end{enumerate}
\end{proof}
\end{Lemme}
\newpage

\bibliographystyle{unsrt-fr}
\bibliography{document_final}
\end{document}
