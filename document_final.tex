\documentclass[a4paper,11pt]{article}
\usepackage[colorlinks=true,linkcolor=black,filecolor=black,urlcolor=black,citecolor=black]{hyperref}
\usepackage[latin1]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage[a4paper]{geometry}
\usepackage{url}


\newtheorem{The}{Théorème}[section]
\newtheorem{Lemme}[The]{Lemme}
\newtheorem{Prop}[The]{Proposition}
\newtheorem{Cor}[The]{Corollaire}
\theoremstyle{definition}
\newtheorem{Def}[The]{Définition}
\newtheorem{Ex}[The]{Exemple}
\theoremstyle{remark}
\newtheorem*{Rq}{Remarque}

\title{Un modèle de croissance aléatoire : le monde découvert par $N$ explorateurs}
\author{Paul MELOTTI et Alexis PRÉVOST \\ sous la direction de Pierre BERTIN}
\date{\today}
\makeatletter
\renewcommand\theequation{\thesection.\arabic{equation}}
\@addtoreset{equation}{section}
\makeatother

\begin{document}
\maketitle
\section*{Introduction}
Nous allons étudier dans ce mémoire l'évolution du monde découvert par des explorateurs dans le cas discret. Le modèle est le suivant : on lance $N$ explorateurs sur $\mathbb{Z}^d$ les uns après les autres. On définit la région explorée récursivement de la façon suivante: on lance le $k^{\mathrm{\grave{e}me}}$ explorateur qui effectue une marche aléatoire symétrique et s'arrête dès qu'il tombe sur une case non explorée (on dit qu'il s'installe). La région explorée à l'instant $k$ est alors la région explorée à l'instant $k-1$ plus la position du $k^{\mathrm{\grave{e}me}}$ explorateur installé. 


Le résultat principal est celui de \cite{ref} qui montre qu'asymptotiquement la région explorée est une boule. On s'interessera ici aux fluctuations autour de cette boule. A.~Asselah et A.~Gaudillière ont montré que celles-ci sont de l'ordre de $\sqrt{\log(n)}$ en dimension 3 ou plus et $\log(n)$ en dimension 2.

Nous allons travailler ici sur l'article \cite{ref2} d'A.~Gaudillière et A.~Asselah qui essaie d'établir l'optimalité des majorations précédentes. Il montre en particulier que $\sqrt{ \log (n)}$ est optimal en dimension supérieure à 3 pour l'erreur interne, mais ne conclut pas pour la dimension 2. En ce qui concerne l'erreur externe, ils prouvent un résultat qui permet de penser que $\sqrt{ \log (n)}$ est optimal en dimension supérieure à $3$ sans tout à fait permettre de conclure.

\begin{center}
 \includegraphics[scale=0.4]{100000.png}
\end{center}


\newpage
\tableofcontents
\newpage

\section{Historique}
Le modèle qui nous intéresse est un exemple de modèle de croissance aléatoire couramment nommé IDLA (Internal Diffusion Limited Aggregation). Il a été introduit en 1986 par P. Meakin et J.M. Deutch \cite{chimie} dans le cadre de la chimie théorique pour décrire des processus comme l'électro-polissage. Dès l'introduction du modèle, il a semblé important de connaître la forme asymptotique de la région explorée obtenue, mais aussi l'ordre des variation par rapport à cette forme limite puisqu'il s'agissait de polir des matériaux.

P. Diaconis et W. Fulton ont formalisé mathématiquement le processus en 1991 \cite{ref4} et démontré certaines de ces propriétés, en particulier une propriété très utile de commutativité dont on fait usage dans la partie \ref{abel}. Le modèle de croissance qu'ils proposent est alors original car il s'agit du premier modèle qui semble donner des régions explorées de forme sphérique.

G. Lawler, M. Bramson, M. et D. Griffeath ont pour la première fois montré en 1992 dans \cite{ref5} que la forme asymptotique de la région explorée était une boule, dans le sens suivant :
on appelle $\mathbb{B}(y,R)=\{x \in \mathbb{Z}^d, \|x-y\| <R\}$ la boule dans $\mathbb{Z}^d$ de centre $y$ et de rayon $R$, et $b(n)=\mathrm{Card}(\mathbb{B}(0,n))$. On note $A(k)$ la région explorée après avoir lancé $k$ explorateurs. Alors :
\begin{The}[Lawler, Bramson, Griffeath]
 Pour tout $\epsilon > 0$, presque sûrement,
 \begin{equation*}
  \mathbb{B}(0,n(1-\epsilon )) \subset A(b(n)) \subset \mathbb{B}(0,n(1-\epsilon ))
 \end{equation*}
 pour tout $n$ assez grand.
\end{The}

Puis, le problème de l'ordre des fluctuations autour de la boule a été étudié par G. Lawler qui montre en 1995 \cite{ref6} que les fluctuations sont presque sûrement majorées par $n^{1/3} \log ^4 (n)$ en dimension $2$ ou plus. Toutefois, des simulations suggèrent que l'ordre des fluctuations est plutôt logarithmique. Aucune amélioration notable de ces résultats n'est publiée, jusqu'en 2010, lorsque A. Asselah et A. Gaudillière montrent dans \cite{ref} que l'erreur est inférieure à $\alpha\sqrt{\log(n)}$ en dimension 3 ou plus et $\alpha \log(n)$ en dimension 2. En parallèle, D.Jerison, L. Levine et S.Sheffield prouvent ce résultat dans \cite{ref8} et \cite{ref7}. Ils publient un second article \cite{ref2} auquel nous nous intéressons ici, qui établit l'optimalité de ce résultat, du moins pour ce qui est de l'erreur interne.

\newpage
\section{Résultats}
\subsection{Définitions}
Commençons par définir ce qu'est la région explorée par $N$ explorateurs :
\begin{Def}
 On appelle explorateur une marche aléatoire symétrique de $\mathbb{Z}^d$ partant d'un point $z$. Tous les explorateurs seront pris indépendants.
 Soit $\Lambda$ la région de $\mathbb{Z}^d$ préalablement explorée et $\xi^n=(\xi_1,\dots,\xi_n)$ les positions de départs des explorateurs qui suivent des marches aléatoires indépendantes $(S_1,\dots,S_n)$ symétriques sur $\mathbb{Z}^d$. 
 On définit alors récursivement $A(\Lambda,\xi^n)$ la région explorée par les explorateurs de la manière suivante :
\begin{itemize}
 \item $A(\Lambda,\emptyset)=\Lambda$
 \item $A(\Lambda,\xi^k)=A(\Lambda,\xi^{k-1})\cup\{S_k(\tau_k)\}$ 
 où $\tau_k$=$\inf \{t\geqslant0 | S_k(t)\notin A(\Lambda,\xi^{k-1})\}$ est le premier temps pour lequel $S_k$ sort de la région explorée. On dit que l'explorateur s'installe à l'instant $\tau_k$.
\end{itemize}

 On notera par la suite $A(\xi^n)=A(\emptyset,\xi^n)$ et $A_R(\Lambda,\xi^n)=A(\Lambda,\xi^n)\cap\mathbb{B}(0,R)$. On notera aussi plus simplement $A(n)=A( (0,\dots,0) )$ où le vecteur est de taille $n$.
 \end{Def}
 
 On va désormais essayer d'estimer la forme asymptotique de cette région explorée pour des explorateurs partant de l'origine, et notamment la différence entre celle-ci et une boule de volume correspondant. On va définir pour cela les notions d'erreur interne et externe.
 
 L'erreur externe, notée $\delta_O$, est la différence entre le tentacule le plus long et le rayon du cercle limite:
 \begin{equation}
  n+\delta_O(n)=\inf \{p\geqslant0\ |\ A(b(n))\subset\mathbb{B}(0,p)\}
 \end{equation}
\\où $b(n)=\mathrm{Card}(\mathbb{B}(0,n))$ est le ``volume'' de la boule de rayon $n$.
L'erreur interne, notée $\delta_I$, est la différence entre le rayon du cercle limite et le trou le plus profond:
 \begin{equation}
  n-\delta_I(n)=\sup \{p\geqslant0\ |\ \mathbb{B}(0,p)\subset A(b(n))\}
 \end{equation}
 Il est important ici de constater que ces deux notions, illustrées sur la figure \ref{ex}, ne sont définies que lorsqu'on lance un nombre d'explorateur de la forme $b(k)$ car il est difficile dans le cas général de connaitre le rayon de la boule asymptotiquement atteinte.

 \subsection{Résultats admis}
  \begin{figure}[H]
 \centering
   \includegraphics{erreur_interne_et_externe.png}
   \caption{Exemple de région explorée par $b(13)$ explorateurs}
   \label{ex}
 \end{figure}
 Le premier résultat concernant les erreurs internes et externes, assure que la région explorée ressemble bien asymptotiquement à une boule.
 \begin{The}[Asselah, Gaudillière]
 \label{limsup}
  En dimension $d \geqslant 3$, $\exists\beta_d>0$ tel que
  \begin{equation}
\limsup\limits_{n\to\infty}\frac{\delta_I(n)}{\sqrt{\log(n)}}\leqslant\beta_d
\ et\ \limsup\limits_{n\to\infty}\frac{\delta_O(n)}{\sqrt{\log(n)}}\leqslant\beta_d   
  \end{equation}
  En dimension 2, $\exists\beta_2>0$ tel que
   \begin{equation}
\limsup\limits_{n\to\infty}\frac{\delta_I(n)}{\log(n)}\leqslant\beta_2
\ et\ \limsup\limits_{n\to\infty}\frac{\delta_O(n)}{\log(n)}\leqslant\beta_2  
  \end{equation}
 \end{The}
 
 Pour illustrer ce résultat, voici une réalisation de $\frac{\delta_I(n)}{\log(n)}$ et $\frac{\delta_O(n)}{\log(n)}$ en dimension 2:
 \begin{figure}[H]
  \begin{minipage}[c]{.46\linewidth}
    \includegraphics{erreurinterne.png}
    \caption{Erreur interne}
  \end{minipage}
  \hfill
 \begin{minipage}[c]{.46\linewidth}
  \includegraphics{erreurexterne.png}
  \caption{Erreur externe}
 \end{minipage}
 \end{figure}

 \subsection{Résultats étudiés}
 On peut naturellement s'interroger sur l'optimalité de cet ordre de fluctuations en $\sqrt{\log(n)}$ pour $d\geqslant3$ et $\log(n)$ pour $d=2$. Dans ce mémoire, nous allons nous interroger sur les deux théorèmes suivants :
 \begin{The}
 \label{ext}
  En dimension $d \geqslant 3$, $\exists\alpha_d$>0 tel que
  \begin{equation}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<h(n))=1
  \end{equation}
  où $h(n)=\alpha_d\sqrt{\log(n)}$.
  \end{The}
  
 \begin{Rq}
  Dans \cite{ref2}, il est également prouvé qu'en dimension $d=2$, $\exists\alpha_2 > 0$ tel que
  \begin{equation}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<h(n))=1
  \end{equation}
  où $h(n)=\alpha_2\sqrt{\log(n)\log(\log(n))}$
 \end{Rq}
Il n'est pas clair \emph{a priori} que ce théorème nous donne vraiment une indication sur l'optimalité de $h(n)$ pour l'erreur externe, mais on peut voir qu'il implique le résultat intéressant suivant :
\begin{Cor}
 \begin{equation} \label{io}
   P(\limsup\limits_{n\to\infty}\{\delta_I(n)\geqslant L(n)\}\cup\{\delta_O(n)\geqslant h(n)\})=1
  \end{equation}
\begin{proof}
On a :
 \begin{align*}
   &P(\exists k\geqslant n,\delta_I(k)\geqslant L(k)\ \normalfont{ou}\ \delta_O(k)\geqslant h(k))
   \\&\geqslant P(\delta_I(n)\geqslant L(n)\ \normalfont{ou}\ \exists k\geqslant n,\delta_0(k)\geqslant h(k))
   \\&= P(\delta_I(n)\geqslant L(n))+P(\exists k\geqslant n,\delta_0(k)\geqslant h(k)\normalfont\ {|}\ \delta_I(n)<L(n))P(\delta_I(n)<L(n))
   \\&= 1-P(\delta_I(n)<L(n))\big(1-P(\exists k\geqslant n,\delta_0(k)\geqslant h(k)\ \normalfont{|}\delta_I(n)<L(n))\big)
 \end{align*}
 et donc :
 \begin{equation*}
 \lim_{n\to\infty}P(\exists k\geqslant n,\delta_I(k)\geqslant L(k)\ \normalfont{ou}\ \delta_O(k)\geqslant h(k))=1
 \end{equation*}
 \end{proof}
\end{Cor}
Autrement dit, pour une infinité de termes, l'erreur interne ou l'erreur externe est d'ordre $\sqrt{\log n}$, donc le théorème \ref{limsup} est d'une certaine façon optimal.

\medskip
Le cas de l'erreur interne est quand à lui complètement déterminé en dimension supérieure à 3 grâce au résultat suivant, qui dit que presque sûrement de grands trous apparaissent une infinité de fois :
 \begin{The} \label{int}
  En dimension $d \geqslant 3$, $\exists\alpha_d>0$ tel que
  \begin{equation}
   P(\limsup\limits_{n\to\infty}\{\delta_I(n)\geqslant h(n)\})=1
  \end{equation}
  où $h(n)=\alpha_d\sqrt{\log(n)}$
 \end{The}

 \newpage

 \section{Propriété abélienne}
 \label{abel}
 On va prouver ici que le choix d'un ordre prédéfini en \ref{iii} sur $\mathbb{Z}^d$ ne change rien à la suite de la démonstration. Le lemme suivant est issu de \cite{ref4} :
 \begin{Lemme}
 Soit $\Lambda$ une partie finie de $\mathbb{Z}^d$, correspondant à la région déjà explorée, et soient $x,y \in \Lambda$. Soit $\Lambda_{x,y}$ la région obtenue en ayant lancé un explorateur depuis $x$, puis un explorateur depuis $y$.
 
 Alors $\Lambda_{x,y}$ a même loi que $\Lambda_{y,x}$.
 \end{Lemme}
 \begin{proof}
 Après avoir lancé deux explorateurs depuis $\Lambda$, on obtient une région de la forme $\Lambda \cup \{w,z \}$ où $w,z \notin \Lambda$. On va montrer que quels que soient $w,z \notin \Lambda$, la probabilité d'obtenir $\Lambda \cup \{w,z \}$ est la même que l'on lance les deux explorateurs depuis $x$ puis $y$ ou depuis $y$ puis $x$.
 
 On utilise la notation suivante : si $x \in \Lambda$ et $w \notin \Lambda$, $p(x,\Lambda,w)$ désigne la probabilité qu'une marche aléatoire lancée depuis $x$ quitte $\Lambda$ exactement en $w$.
 
 Si on commence par lancer depuis $x$, on obtient l'ensemble $\Lambda \cup \{w\}$ avec probabilité $p(x,\Lambda,w)$ et l'ensemble $\Lambda \cup \{w\}$ avec probabilité $p(x,\Lambda,z)$. On lance ensuite le second explorateur depuis $y$, et on obtient l'ensemble $\Lambda \cup \{w,z \}$ avec probabilité
 \[ p(x,\Lambda,w) p(y,\Lambda\cup \{w\},z) + p(x,\Lambda,z) p(y,\Lambda\cup \{z\},w) \]
 En distinguant le fait que la marche partant de $y$ passe ou non par $w$, on remarque que
 \[p(y,\Lambda\cup \{w\},z) = p(y,\Lambda,z) + p(y,\Lambda,w)p(w,\Lambda\cup \{w\},z) \]
 Donc la probabilité d'obtenir $\Lambda \cup \{w,z \}$ est
 \begin{align*}
  &p(x,\Lambda,w) (p(y,\Lambda,z) + p(y,\Lambda,w)p(w,\Lambda\cup \{w\},z)) \\+& p(x,\Lambda,z) (p(y,\Lambda,w) + p(y,\Lambda,z)p(z,\Lambda\cup \{z\},w))
  \\=&p(x,\Lambda,w)p(y,\Lambda,z)(2+p(w,\Lambda\cup \{w\},z)+p(z,\Lambda\cup \{z\},w))
 \end{align*}
 qui est symétrique en $x$ et $y$, donc est indépendante de l'ordre des deux marches.
 
 Ceci étant vrai pour tous $w$ et $z$ hors de $\Lambda$, on en déduit que $\Lambda_{x,y}$ et  $\Lambda_{y,x}$ ont même loi.
 \end{proof}
 
 En itérant cette propriété, on obtient le théorème suivant, qu'on appelle propriété abélienne :
\begin{The}
Soit n un entier naturel, $\Lambda$ une partie de $\mathbb{Z}^d$, $\xi=(\xi_1,\dots,\xi_n)$ et $\zeta=(\zeta_1,\dots,\zeta_n)$ deux vecteurs de $\left(\mathbb{Z}^d\right)^n$ égaux à permutation des coordonnées près. On a alors l'égalité en loi : 
\begin{equation*}
 A(\Lambda,\xi)=A(\Lambda,\zeta)
\end{equation*}
\end{The}

\pagebreak[2]
\begin{Def}
Pour $y\in\mathbb{Z}^d$ et R>0, on définit le bord de la boule de centre y et de rayon R par :
 \[\partial \mathbb{B}(y,R)=\{x \notin \mathbb{B}(y,R) \ | \ \exists z \in \mathbb{B}(y,R), \ \| x-z\| <1\}\]
\end{Def}

Illustrons ce théorème avec l'exemple de l'exploration par vague qui nous servira par la suite. On va réaliser la région explorée $A(n+m)$ en $3$ vagues. La première vague consiste à faire partir $n$ explorateurs de l'origine et à considérer la région explorée correspondante $A_1=A(n)$ (en bleu sur la figure \ref{vague}). 

Pour la deuxième vague, on fait partir $m$ explorateurs de l'origine avec pour région préalablement explorée $A_1$ et on les arrête sur $\mathbb{B}(0,R)$ (en vert sur \ref{vague}). On note $A_2$ la région explorée après les deux vagues et $\Delta_R$ la configuration des explorateurs de la deuxième vague arrêtés sur $\partial\mathbb{B}(0,R)$ :
\begin{equation*}
 A_2=A_1\cup A_R(A_1,\zeta_0^N) 
 \qquad\Delta_R=\{\zeta_z^{W_R(A_1;\zeta_0^m,z)},z\in\partial\mathbb{B}(0,R)\}
\end{equation*}

Pour la troisième vague on relance les explorateurs arrêtés $\Delta_R$ avec pour région préalablement explorée $A_2$ (en noir sur la figure \ref{vague}) et on pose $A_3=A_2\cup A(A_2;\Delta_R)$. La propriété abélienne nous donne alors l'égalité en loi :
\begin{equation*}
 A_3=A(n+m)
\end{equation*}
\begin{figure}[H]
 \centering
   \includegraphics{vague.jpg}
   \caption{La région explorée par vagues}
\label{vague}
\end{figure}
 \newpage
 
 \section{L'erreur externe}
 \label{eext}
 On va prouver ici le théorème \ref{ext} : en dimension supérieure ou égale à 3,
\begin{equation*}
   \lim_{n\to\infty}P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<L(n))=1
\end{equation*}

Introduisons dès à présent quelques définitions qui nous seront utiles par la suite.
\begin{Def}Soit $\Lambda$ une partie de $\mathbb{Z}^d$, S une marche aléatoire discrète, y un point de $\mathbb{Z}^d$ et $\gamma$ et $R$ des réels positifs.
\begin{enumerate}[i)]

  \item $\rho(\gamma)=\sup\{n\geqslant 0,\ b(n)\leqslant \gamma\}$ est le rayon de la plus grande boule de volume inférieur à $\gamma$.
  
  \item $H(S;\Lambda)=\inf\{n\geqslant0,\ S(n)\in\Lambda\}$ (ou $H(\Lambda)$ en l'absence d'ambiguïté sur $S$) est le premier instant pour lequel la marche aléatoire $S$ atteint $\Lambda$.
  
  \item \label{iii}Si $\eta:\mathbb{Z}^d\to\mathbb{N}$ est telle que $\eta(z)$ est le nombre d'explorateurs partis de $z$, la région explorée en partant de $\eta$ sera la région définie en faisant partir les explorateurs dans un ordre prédéfini sur $\mathbb{Z}^d$. 
  
  \item $W_R(\Lambda;\eta, z)$ est le nombre d'explorateurs, partant de la configuration $\eta$ et de la région explorée $\Lambda$, qui passent par $z\in\mathbb{B}(0,R)$ (resp $z\in\partial\mathbb{B}(0,R)$) avant de sortir de $\mathbb{B}(0,R)$ (resp au moment où ils sortent de $\mathbb{B}(0,R)$).
 
  \item Pour $z \in \mathbb{Z}^d \setminus \{ 0 \}$ on note $\Sigma (z) = \partial \mathbb{B} (0, \| z \| )$. Remarquons que $z \in \Sigma (z)$.
  
  \item $\zeta_z^N$ est le vecteur $(z,\dots,z)$ de taille $N$.
\end{enumerate}
 \end{Def}
 \begin{Rq}
  Si $\mathbb{B}(0,R)\subset\Lambda$, $W_R(\mathbb{B}(0,R);\eta,z)=W_R(\Lambda;\eta,z)$. On appelle alors cette quantité $M_R(\eta,z)$.
 \end{Rq}
\subsection{Conditions d'apparition d'un tentacule}
 On pose $h(n)=\alpha\sqrt{\log(n)}$ et $L(n)=\gamma\sqrt{\log(n)}$ où $\alpha$ et $\gamma$ sont des constantes à choisir par la suite. $h$ représente ici l'erreur externe et $L$ l'erreur interne, et il est important de conserver cette distinction à l'esprit même si on prendra $\alpha=\gamma$ par la suite.

Pour estimer l'erreur externe, on va commencer par minorer la probabilité de fabriquer un tentacule qui sorte de $\mathbb{B}(0,R)$ lorsqu'on fait  partir $N$ explorateurs de $z$, avec $\|z\|<R$ (on prendra par la suite $R$ de la forme $\|z\|+h(n)$).
\begin{Prop}
\label{step2}
 Il existe $c>1$ tel que $\forall z\in\mathbb{Z}^d$, si on prend $R$ tel que $N\geqslant c(R-\|z\|)$ et $R\geqslant\|z\|+1$, on ait, pour tout $\Lambda\subset\mathbb{Z}^d$,
 \begin{equation*}
  P(A(\Lambda,\zeta_z^N)\not\subset\mathbb{B}(0,R))\geqslant \exp (-c(R-\|z\|)^2)
 \end{equation*}
\begin{proof}
 On commence par remarquer qu'on peut prendre $\Lambda$=$\emptyset$ car \newline $A(\emptyset,\xi)\subset A(\Lambda,\xi) \Rightarrow P(A(\Lambda,\xi)\not\subset\mathbb{B}(0,R))\geqslant P(A(\emptyset,\xi)\not\subset\mathbb{B}(0,R))$.
 Le reste de la démonstration consiste à créer un tentacule qui sort de $\mathbb{B}(0,R)$ en partant de $z$. Pour cela posons 
 \begin{equation*}
  x_n=(\|z\|+n)\frac{z}{\|z\|}
 \end{equation*}
La suite $x_n$ est une suite de points partant de $z$ et qui arrive sur $\mathbb{B}(0,R)$ perpendiculairement à celle-ci. Pour tout cube unitaire centré en $x_n$, il existe $z_n\in\mathbb{Z}^d$ dans ce cube. Soit $p$ le plus petit entier tel que $\|x_p\|\geqslant R+\sqrt{d}$, alors $\|z_p\|\geqslant R$. 
On définit alors $(y_1=z,y_2,\dots,y_k=z_p)$ une suite de plus proches voisins dans $\mathbb{Z}^d$, où l'on va chercher à prendre $k$ suffisamment petit. 

On peut trouver une constante $c_1$ qui permet de prendre $k\leqslant c_1p$. Pour cela, on procède de la façon suivante: on appelle $C$ le diamètre de la figure composée de deux cube unitaire partageant une face, de façon à avoir $\|z_n-z_{n-1}\|\leqslant C$. Il est alors possible de relier $z_n$ à $z_{n-1}$ par une suite de plus proches voisins de taille inférieure à $c_1=dC$, ce qui prouve le résulat.
On alors $k\leqslant c_1(R-\|z\|+\sqrt{d}+1) \leqslant c(R- \|z\|)$ pour $c$ une constante suffisamment grande. Comme $N \geqslant c(R-\|z\|)$, on peut forcer les $k$ premiers explorateurs à s'installer successivement sur $y_1, \dots, y_k$, et la probabilité de cet événement est :
\begin{equation*}
 (\frac{1}{2d})^{\sum_{n=1}^{k}{n}}\geqslant \exp(-c(R-\|z\|)^2)
\end{equation*}
quitte à augmenter la constante $c$. La présence d'un tel tentacule impliquant que les explorateurs sortent de $\mathbb{B}(0,R)$, on a bien le résultat voulu.
\end{proof}
 \end{Prop}
 
 \subsection{Construction d'un tentacule}
 On fixe désormais l'entier $n$, et on va essayer d'estimer la probabilité de trouver un entier $k\geqslant n$ pour lequel l'erreur externe dépasse $h(k)$. Pour cela on va utiliser un processus par vague comme décrit dans la section \ref{abel} en lançant les explorateurs de l'origine en trois vagues :
 
 La première est consituée de $b(n)$ explorateurs partis de l'origine qui explorent $A_1$, la deuxième de $X_n$ explorateurs arrêtés partis de l'origine, où $X_n$ est une variable aléatoire discrète indépendante des marches des explorateurs dont la loi sera fixée par la suite, et la troisième est constituée des explorateurs de la seconde vague relancés. Les explorateurs de la deuxième vague sont arrêtés sur $\Sigma=\partial\mathbb{B}(0,r_n)$ avant de les faire repartir, avec $r_n = n-L(n)$. 
 
 La propriété abélienne montre alors que la région explorée de cette façon est la même, en loi, que celle explorée en lançant directement $X_n+b(n)$ explorateurs. On cherche à trouver $X_n$ tel que l'évènement $\delta_O(R_n)\geqslant h(R_n)$ où $R_n=\rho(b(n)+X_n)+1$ arrive avec une probabilité tendant vers 1.

\medskip
Après avoir arrêté la deuxième vague sur $\Sigma$, on considère l'événement : partant d'un point $z$ de $\Sigma$, en relançant les explorateurs arrêtés en $z$, on construit un tentacule qui sort de $\mathbb{B}(0,R_n+h(R_n))$. On note cet évènement $\mathrm{cov}(z)$ :
\begin{equation}
 \mathrm{cov}(z)=\{A(\Lambda_n,\zeta_z^{N_z})\not\subset\mathbb{B}(0,R_n+h(R_n))\}
\end{equation}
où $N_z=W_{r_n}(A_1;\zeta_0^{X_n},z)$ est le nombre d'explorateurs arrétés en $z$ et $\Lambda_n$ est la région explorée au moment où on a arrété la deuxième vague. Les cov(z) ne sont pas indépendants pour le moment.
\medskip

\begin{figure}[H]
 \centering
   \includegraphics{cov.png}
   \caption{Un exemple où $\mathrm{cov}(z)$ est réalisé}
\end{figure}
\medskip
\pagebreak[2]

On constate que l'évènement $\delta_O(R_n)\geqslant h(R_n)$ est lié à $\{\mathrm{cov}(z), z\in\Sigma\}$ de la façon suivante :
\begin{align}
\nonumber P(\delta_O(R_n)\geqslant h(R_n))&\geqslant P(A(b(R_n))\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(A(b(n)+X_n)\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(A(\Lambda_n,\sum_{z\in\Sigma}\zeta_z^{N_z})\not\subset\mathbb{B}(0,R_n+h(R_n)))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov}(z))
\end{align}

Il nous faut désormais estimer $P(\bigcup_{z\in\Sigma}\mathrm{cov}(z))$. Pour cela, il serait intéressant d'avoir indépendance entre les $\mathrm{cov}(z)$ et ce qui se passe avant sous certaines hypothèses, ce qu'on va obtenir en prenant pour $X_n$ une variable de Poisson. En effet, les variables de Poisson ont la propriété de découpage \ref{indé}, qui laisse penser que les $N_z$ seront indépendantes et de loi connue.

Cette propriété ne permet pas pour autant d'affirmer que les $N_z$ sont indépendantes de ce qui se passe avant pour $z \in \Sigma$, mais cela sera vrai lorsque $\delta_I(n) < L(n)$. On définit en effet pour tout $z\in\Sigma$
\begin{equation}
 \mathrm{cov_2}(z)=\{A(\Delta_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,R_n+h(R_n))\}
\end{equation}
où $N'_z=W_{r_n}(\Delta_n;\zeta_0^{X_n},z)$ est le nombre d'explorateurs de la deuxième vague qui touchent $\Sigma$ en $z$ lorsque la région explorée initiale est $\Delta_n=\mathbb{B}(0,r_n)$.
\begin{figure}[H]
 \centering
   \includegraphics{cov2.png}
   \caption{Un exemple où $\mathrm{cov_2}(z)$ est réalisé}
\end{figure}
On remarque que la seule différence entre $\mathrm{cov}(z)$ et $\mathrm{cov_2}(z)$ est la région explorée avant de lancer la deuxième vague. C'est ici qu'il est important de se placer dans le cas $\delta_I(n)<L(n)$, et on restera dans ce cas jusqu'à la fin de cette partie, puisqu'on a alors:
\begin{equation*}
 W_{n-L(n)}(A(\zeta_0^{b(n)});X_n1_0,z)=M_{n-L(n)}(X_n1_0,z)
\end{equation*}
et donc $N'_z=N_z$ et $\mathrm{cov_2}(z) \subset \mathrm{cov}(z)$. 

Soit $U$ la variable aléatoire à valeur dans $\Sigma$ calculée en regardant en quel point une marche aléatoire symétrique atteint $\Sigma$. On alors :
\begin{equation*}
 N'_z=\sum_{p\leqslant X_n}{1_{U_p=z}}
\end{equation*}
D'après la proposition \ref{indé} les $N'_z$ sont alors des variables de Poisson indépendantes, et donc les $\mathrm{cov_2}(z)$ sont indépendants. Ils sont de plus indépendants de $\delta_I(n)$ puisque celui-ci ne dépend que de la première vague alors que $\mathrm{cov_2}$ en est indépendant. Résumons ce qu'on a fait jusqu'à présent :

\begin{align}
\label{16}
\nonumber P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\ \normalfont{|}\ \delta_I(n)<L(n))&\geqslant P(\delta_0(R_n)\geqslant h(R_n)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov}(z)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z)\ \normalfont{|}\ \delta_I(n)<L(n))
 \nonumber\\&=P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z))
\end{align}
 \subsection{Estimation de l'erreur externe}
Il faut maintenant trouver $\lambda_n$, le paramètre de la loi de Poisson $X_n$, pour que tout ceci tende vers $1$.
La technique consiste à majorer $R_n+h(R_n)$ de façon à pouvoir appliquer la proposition \ref{step2}:
\begin{Lemme}
\label{pois}
Pour $n$ suffisamment grand,
 \begin{equation}
  X_n\leqslant b(n+h(n))-b(n)\Rightarrow R_n+h(R_n)\leqslant n+3h(n) 
 \end{equation}
 \begin{proof}
  \begin{align*}
   X_n\leqslant b(n+h(n))-b(n)\Rightarrow X_n+b(n)\leqslant b(n+h(n))&\Rightarrow R_n-1\leqslant n+h(n)
   \\&\Rightarrow h(R_n)\leqslant 2h(n)-1
  \end{align*}
  pour $n$ suffisamment grand, d'où le résultat.
 \end{proof}
\end{Lemme}

On prend alors $\lambda_n=\frac{b(n+h(n))-b(n)}{2}$ pour pouvoir se placer dans le cas du lemme précédent sans difficulté, en effet, on aura souvent $X_n$ plus petit que $2\lambda_n$ : d'après la propriété annexe \ref{deuxmoy}, 
\begin{equation}
\lim_{n \to \infty} P(X_n < 2\lambda_n) = 1
\end{equation}

En utilisant le lemme \ref{pois}, on a donc :
\begin{align}
\label{17}
\nonumber P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z))&\geqslant P(\bigcup_{z\in\Sigma}\mathrm{cov_2}(z)\cap\{X_n\leqslant\lambda_n\})
\nonumber\\&\geqslant P(\bigcup_{z\in\Sigma}\{A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n))\})P(X_n\leqslant\lambda_n)
\nonumber\\&=P(X_n\leqslant\lambda_n)\big(1-\prod_{z\in\Sigma}\big(1-P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n)))\big)\big)
\end{align}
Or, d'après la proposition \ref{step2},
\begin{align}
\label{18}
\nonumber &P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n)))
\nonumber\\&=P(A(\Lambda_n,\zeta_z^{N'_z})\not\subset\mathbb{B}(0,n+3h(n))\ \normalfont{|}\ N'_z\geqslant c(3h(n)+L(n)))P(N'_z\geqslant c(3h(n)+L(n)))
\nonumber\\&\geqslant \exp(-c(3h(n)+L(n))^2)P(N'_z\geqslant c(3h(n)+L(n)))
\end{align}

Il ne reste donc plus qu'à minorer $P(N'_z\geqslant c(3h(n)+L(n)))$:
\begin{Lemme}
 $\forall c_1>0, \exists C\geqslant 0$ tel que
 \begin{equation}
 \label{11}
  P(N'_z\geqslant c_1h(n))\geqslant \exp (-Ch(n))
 \end{equation}
 \begin{proof}
  D'après la proposition \ref{indé}, $N'_z$ est une variable de poisson de paramètre $\mu=\lambda_n P(S(H(\Sigma))=z)$ où S est une marche aléatoire symétrique sur $\mathbb{Z}^d$ partant de $0$. En utilisant le lemme 1.7.4 de \cite{ref3}, on a des constantes $c_2$ et $c_3$ telles que :
  \begin{equation*}
   \frac{c_2}{(n-L(n)+1)^{d-1}}\leqslant\frac{c_2}{\|z\|^{d-1}}\leqslant P(S(H(\Sigma))=z)\leqslant\frac{c_3}{\|z\|^{d-1}}\leqslant\frac{c_3}{(n-L(n))^{d-1}}
  \end{equation*}
 De plus, d'après le lemme \ref{b(n)}, $c_4h(n)n^{d-1}\leqslant\lambda_n\leqslant c_5h(n)n^{d-1}$, donc $c_6 h(n) \leqslant\mu\leqslant c_7 h(n)$. On a donc, en posant $a=\lfloor c_1 h(n) \rfloor +1$ :
 \begin{align*}
  P(N'_z\geqslant c_1h(n))&\geqslant P(N'_z=a)=e^{-\mu}\frac{\mu^a}{a!}
  \\&\geqslant e^{-c_7h(n)}\frac{(c_6h(n))^{c_1h(n)}}{a!}
  \\&\sim e^{-c_7h(n)}\frac{(c_6h(n))^{c_1h(n)}}{\sqrt{2\pi a}}\Big(\frac{e}{c1h(n)}\Big)^{[c_1h(n)]+1}
  \\&\geqslant  e^{(-c_7+c_1(1-ln(c_1)+ln(c_6)))h(n)+2}\frac{1}{h(n)^2\sqrt{2\pi a}}
  \\&\geqslant \exp(-Ch(n))
 \end{align*}
 \end{proof}
\end{Lemme}

En prenant $h(n)=L(n)$ et en utilisant (\ref{16}), (\ref{17}), (\ref{18}), (\ref{11}) et que $1-x\leqslant e^{-x}$, on a finalement :
\begin{align}
 \nonumber&P(\exists k\geqslant n,\ \delta_0(k)\geqslant h(k)\normalfont\ {|}\ \delta_I(n)<L(n))
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)\big(1-\prod_{z\in\Sigma}\big(1-\exp(-c(4h(n))^2-Ch(n))\big)\big)
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1}\exp(-Ch(n)-c'h(n)^2)))
 \nonumber\\&\geqslant P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1}\exp(-c''h(n)^2)))
 \nonumber\\&=P(X_n\leqslant\lambda_n)(1-\exp(-\kappa n^{d-1-\alpha^2c''}))
\end{align}

Il suffit maintenant de fixer $\alpha$ suffisamment petit pour que $d-1-\alpha^2 c''$ soit positif pour terminer la preuve du théorème \ref{ext}.
\medskip
\pagebreak[1] %n'efface pas cette commande elle permet de «suggérer» à Latex de sauter une page ici, sans l'obliger, pour avoir un meilleur rendu.

\section{L'erreur interne}
On reprend dans cette partie les définitions introduites au début de la partie \ref{eext}.

\label{interne}
Pour établir le Théorème \ref{int}, nous allons procéder de façon similaire. Nous lançons cette fois-ci non pas une vague d'explorateurs à partir de $A(b(n))$, mais un grand nombre $F(n)$ de vagues successives. Pour $k \leqslant F(n)$, la $k^{\mathrm{\grave{e}me}}$ vague consistera en un lancer de $X_k$ explorateurs depuis l'origine, où $X_k$ sera une varible de Poisson de paramètre $\lambda_k$ à déterminer plus tard.

On note $R_k$ le rayon théorique après la $k^{\mathrm{\grave{e}me}}$ vague :
\begin{equation*}
 R_k = \rho (b(n) + X_1 + \dots + X_k)
\end{equation*}

Et $A_k$ l'évènement qui nous intéresse :
\begin{equation*}
 A_k = \{ \delta_I (R_k) > h(R_k) \} 
\end{equation*}

On note aussi $B_k$ son analogue pour l'erreur externe :
\begin{equation*}
 B_k = \{ \delta_O (R_k) > L(R_k) \} 
\end{equation*}

Le but sera de montrer que 
\begin{equation}
 \lim _{n \to \infty } P \left( \bigcup _{k \leqslant F(n)} A_k \right) = 1
\end{equation}

Une propriété simple nous sera utile par la suite :
\begin{Rq}
 Quel que soit $k \leqslant F(n)$, on a toujours :
 \begin{equation} \label{nonvide}
  A(b(R_k))^c \cap (\mathbb{B} (0, R_k)\cup\partial\mathbb{B} (0, R_k)) \neq \emptyset
 \end{equation}
 En effet, le cas échéant, on aurait $(\mathbb{B} (0, R_k)\cup\partial\mathbb{B} (0, R_k)) \subset A(b(R_k))$ et donc strictement plus de $b(R_k)$ sites seraient occupés par les explorateurs, alors qu'on n'a lancé que $b(R_k)$ explorateurs.
\end{Rq}

\subsection{Lorsque $A^c_{k-1}$ est réalisé}
On suppose dans cette partie que l'évènement $A^c_{k-1}$ est réalisé, pour $1 \leqslant k \leqslant F(n)-1$. Utilisons alors l'ensemble non vide donné par (\ref{nonvide}) au rang $k-1$ : il existe un $Z_k$ non encore visité, tel que $\|Z_k\| \leqslant R_{k-1}+1$. Comme $A^c_{k-1}$ est réalisé et que $Z_k\in A(b(R_{k-1}))^c$, nécessairement, 
\begin{equation} \label{zk}
 R_{k-1} - h(R_{k-1}) \leqslant \|Z_k\| \leqslant R_{k-1} + 1.
\end{equation}
On va s'appuyer sur ce $Z_k$ pour construire un évènement $I_k$ qui pourra s'interpréter par : soit $Z_k$ est un trou laissé à la prochaine vague $k$, soit un long tentacule apparaît à la vague $k$.

\begin{Def}
\begin{enumerate}[i)]
 \item
 Soit $n$ un entier et $R > 0$, on définit la variable aléatoire $N_z (n, R)$ comme le nombre de marches aléatoires parmi $n$ marches partant de $0$ qui visitent $z$ avant de sortir de $\mathbb{B} (0, \| z \| + R)$ :
\begin{equation}
 N_z (n, R) = \mathrm{Card} \{ i \leqslant n \ \mid \ H(z) < H( \mathbb{B}^c (O, \| z \| + R )) \}
\end{equation}
 \item Soit $\Lambda_k = \mathbb{B}(Z_k, L(R_k)) \cap \Sigma (Z_k)$ et $\Lambda'_k = \Sigma (Z_k) \setminus \Lambda_k$ et :
 \begin{equation}
  I_k = \{ N_{Z_k} (X_k, 7 L(R_{k-1})) = 0 \}
 \end{equation}
\end{enumerate}
\end{Def}

Si $I_k$ est réalisé, on aimerait pouvoir dire que dans le cas où $Z_k$ n'est pas visité (un trou apparaît) on a $\delta_I (R_k) \geqslant h(R_k)$ et que dans le cas contraire (un long tentacule apparaît) on a $\delta_O (R_k) \geqslant L(R_k)$. Pour cela, il faut montrer que sous certaines conditions sur $X_k$ on a $R_k - \| Z_k \| \geqslant h(R_k)$ et $\| Z_k \| - R_k + 7 L(R_{k-1}) \geqslant L(R_k)$.

Cela sera assuré pour $X_k$ dans un bon intervalle, donné par le lemme suivant, qui induira du coup le choix du paramètre $\lambda_k$ :
\begin{Lemme}
Pour $k$ suffisamment grand, si $\mu_k = b(R_{k-1} + 2 h(R_{k-1})) - b(R_{k-1})$,
\begin{eqnarray*} \frac{2}{3} \mu_k \leqslant X_k \leqslant 2 \mu_k & \Rightarrow & (i) \ R_k - \| Z_k \| \geqslant h(R_k)
\\ &\mathrm{et}& (ii) \ \| Z_k \| - R_k + 7 L(R_{k-1}) \geqslant L(R_k)
\end{eqnarray*}
\end{Lemme}
\begin{proof}
 D'une part,
  \begin{eqnarray*}
   X_k\leqslant 2(b(R_{k-1} + 2 h(R_{k-1})) - b(R_{k-1})) & \Rightarrow & X_k + b(R_{k-1}) \leqslant b(R_{k-1} + 4 h(R_{k-1}))
   \nonumber\\&\Rightarrow & R_{k}\leqslant R_{k-1}+4h(R_{k-1}) \label{r1}
  \end{eqnarray*}
 D'autre part,
  \begin{eqnarray*}
   X_k\geqslant \frac23 (b(R_{k-1} + 2 h(R_{k-1})) - b(R_{k-1})) & \Rightarrow & X_k + b(R_{k-1}) \geqslant b(R_{k-1} + \frac43 h(R_{k-1})) 
   \nonumber\\&\Rightarrow & R_{k}\geqslant R_{k-1}+\frac43 h(R_{k-1}) \label{r2}
  \end{eqnarray*}
  
  On en déduit que
  \begin{eqnarray*}
   R_k - \| Z_k \| & \geqslant & R_k - R_{k-1} - 1 \ \ \mbox{par (\ref{zk})}
   \nonumber\\ & \geqslant & \frac43 h(R_{k-1}) - 1 \ \ \mbox{par (\ref{r2})}
   \nonumber\\ & \geqslant & h(R_{k}) \ \ \mbox{par (\ref{r1}), pour $k$ assez grand,}
   \nonumber
  \end{eqnarray*}
  ce qui prouve $(i)$.
  
  Pour le $(ii)$ on a :
  \begin{eqnarray*}
   \| Z_k \| - R_k + 7 L(R_{k-1}) & \geqslant & R_{k-1} - R_{k} +7L(R_{k-1}) - h(R_{k-1}) \ \ \mbox{par (\ref{zk})}
   \nonumber\\ & \geqslant & -5 h(R_{k-1}) + 7 L(R_{k-1}) \ \ \mbox{par (\ref{r2})}
   \nonumber\\ & \geqslant & L(R_{k}) \ \ \mbox{par (\ref{r1}), pour $k$ assez grand.}
   \nonumber
  \end{eqnarray*}
\end{proof}On voit donc qu'il est judicieux de prendre $\lambda_k = \mu_k = b(R_{k-1} + 2 h(R_{k-1})) - b(R_{k-1})$. Ainsi, lorsque la variable de Poisson $X_k$ est comprise entre $\frac{2}{3}$ et $2$ fois sa moyenne, on a bien $I_k \subset A_k \cup B_k$.

Enfin, on a besoin de s'assurer que la probabilité de $I_k$ est assez grande, ce qui est donné par le lemme technique suivant :

\begin{Lemme}
 Il existe une constante $C > 0$ telle que, si $X$ une variable de Poisson de paramètre $\lambda$, $z$ un point de $\in \mathbb{Z}^d \setminus \{ 0 \}$ et $R > 0$, 
 \begin{equation*}
  P(N_z (X, R) = 0) \geqslant \exp \left( - C \frac{\lambda R}{\| z \| ^{d-1} } \right)
 \end{equation*}
\end{Lemme}
\begin{proof}
 Notons $\Lambda = \mathbb{B} (z, R) \cap \Sigma (z)$ et $\Lambda ' = \Sigma (z) \setminus \Lambda$. Notons aussi :
 \[ N_z (n, \Lambda, R) = \mathrm{Card} \{ i \leqslant n \ \mid \ S_i (H(\Sigma (z))) \in \Lambda, \ H(z) < H( \mathbb{B}^c (O, \| z \| + R )) \} \]
 c'est-à-dire le nombre de marches aléatoires parmi les $N_z (n, R)$ qui atteignent $\Sigma (z)$ en un point de $\Lambda$.
 
 D'après la proposition \ref{indé}, $N_z (X, \Lambda, \infty )$ et $N_z (X, \Lambda ', R)$ sont des variables de Poisson indépendantes. Ainsi :
\begin{eqnarray*} P(N_z(X,R) = 0) & \leqslant & P(N_z (X, \Lambda, \infty ) = 0, N_z (X, \Lambda ', R) = 0) 
\\ & = & \exp \left( - E[N_z (X, \Lambda, \infty )] - E[N_z (X, \Lambda ', R)] \right)
\end{eqnarray*}

On va montrer que chacune de ces espérances est majorée par un terme du type $c \frac{\lambda R}{\| z \| ^{d-1} }$.
\begin{itemize}
 \medskip
 \item Pour la première, on se sert du fait que $P_0 (S(H(\Sigma (z))) = y) \leqslant \frac{c_1} {\| z \| ^ {d-1}} $ (Lemme 1.7.4 de \cite{ref3}) et $P_y (H(z) < \infty ) \leqslant \frac{c_2}{1+\| y-z \| ^{d-2}}$ (Théorème 1.5.4 de \cite{ref3}), ainsi que de la propriété de Markov fort, pour écrire :
 \begin{eqnarray}
 E[N_z (X, \Lambda, \infty )] & = & \lambda \ P_0 (S(H(\Sigma (z))) \in \Lambda, \ H(z) < \infty)
 \nonumber\\& = & \lambda \ P_0 (S(H(\Sigma (z))) = z)
 \nonumber\\ & & + \lambda \sum _{y \in \Lambda \setminus \{z\} } P_0 (S(H(\Sigma (z))) = y, \ H(z) < \infty )
 \nonumber\\& = & \lambda \ P_0 (S(H(\Sigma (z))) = z)
 \nonumber\\ & & + \lambda \sum _{y \in \Lambda \setminus \{z\} } P_0 (S(H(\Sigma (z))) = y) \ P_y (H(z) < \infty )
 \nonumber\\& \leqslant & \lambda \frac{c_1} {\| z \| ^ {d-1}} \left( 1 + \sum _{y \in \Lambda \setminus \{z\} } \frac{c_2}{1+\| y-z \| ^{d-2}} \right) \nonumber
 \end{eqnarray}
 \pagebreak[1]
 
 En arrangeant plutôt la somme selon $k = \lfloor \| y-z \| \rfloor$, et en observant que pour un $k$ fixé on a moins de $c_3 k^{d-2}$ positions de $y$ correspondants, on obtient :
 \begin{eqnarray}
 E[N_z (X, \Lambda, \infty )] & \leqslant & c_4 \frac{\lambda} {\| z \| ^ {d-1}} \left( 1 + \sum _{k=1} ^{R} \frac{k^{d-2}}{1+k^{d-2}} \right)
 \nonumber\\& \leqslant & c_5 \frac{\lambda R} {\| z \| ^ {d-1}} \nonumber
 \end{eqnarray}
 
 \item Pour la seconde espérance, on utilise le résultat du Lemme 5(b) de \cite{ref7} qui assure que $P_y ( H(z) < H( \mathbb{B}^c (0, \| z \| + h )) ) \leqslant \frac{c_6 R^2}{\| z-y \| ^d}$, et on utilise la même transformation sur la somme :
 \begin{eqnarray}
 E[N_z (X, \Lambda ', R)] & = & \lambda \ P_0 (S(H(\Sigma (z))) \in \Lambda ', \ H(z) < H( \mathbb{B}^c (0, \| z \| + h )) )
 \nonumber\\& = & \lambda \sum _{y \in \Lambda '} P_0 (S(H(\Sigma (z))) = y) \ P_y (H(z) < H( \mathbb{B}^c (0, \| z \| + h )) )
 \nonumber\\& \leqslant & \lambda \sum _{y \in \Lambda '} \frac{c_1} {\| z \| ^ {d-1}} \ \frac{c_6 R^2}{\| z-y \| ^d}
 \nonumber\\& \leqslant & c_7 \frac{\lambda R^2} {\| z \| ^ {d-1}} \sum _{k=R} ^{2\|z\|+2} \frac{k^{d-2}}{k^{d}}
 \nonumber\\& \leqslant & c_7 \frac{\lambda R^2} {\| z \| ^ {d-1}} \sum _{k=R} ^{\infty} \frac{1}{k^{2}}
 \nonumber\\& \leqslant & c_8 \frac{\lambda R} {\| z \| ^ {d-1}} \nonumber
 \end{eqnarray}
 où la dernière inégalité provient d'une comparaison série-intégrale sur la somme.
\end{itemize}
\end{proof}
En appliquant ce lemme à $X_k$, $Z_k$ et $L(R_{k-1})$, on obtient :
\begin{eqnarray}
 P(I_k) & \geqslant & \exp \left( - C' \frac{\lambda_k L(R_{k-1}) }{\| Z_k \| ^{d-1} } \right)
 \nonumber\\& \geqslant & \exp \left( - C' \frac{\lambda_k L(R_{k-1}) }{(R_{k-1} - h(R_{k-1}))^{d-1} } \right)
 \nonumber\\& \geqslant & \exp \left( - C' \frac{c_1 R_{k-1}^{d-1}h(R_{k-1})L(R_{k-1}) }{(R_{k-1} - h(R_{k-1}))^{d-1} } \right)
 \nonumber\\& \geqslant & \exp \left( - \kappa h(R_{k-1}) L(R_{k-1}) \right)
 \nonumber\\& \geqslant & \exp \left( - \kappa h(R_{F(n)}) L(R_{F(n)}) \right)
 \label{pik}
\end{eqnarray}
car $\lambda_k$ est de l'ordre de $R_{k-1}^{d-1}h(R_{k-1})$ d'après le lemme \ref{b(n)} et $R_{k-1} - h(R_{k-1})$ est de l'ordre de $R_{k-1}$.
 
\subsection{Minorations}
Résumons ce qu'on a fait jusqu'à présent : on a montré qu'il existait un évènement $I_k$ tel que $\frac{2}{3} \lambda_k \leqslant X_k \leqslant 2 \lambda_k \Rightarrow A_{k-1}^c \cap I_k \subset A_k \cup B_k$ et $P(I_k) \geqslant \exp 
\left( - \kappa h(R_{F(n)}) L(R_{F(n)}) \right)$. Notre but est de montrer que $\lim_{n \to \infty } P \left( \bigcup _{k \leqslant F(n)} A_k \right) = 1$. La fonction $F(n)$ est toujours à choisir.

On note $\mathcal{G}_k$ la filtration associée à l'exploration par vagues. On note $C_k$ l'évènement $\{ \frac{2}{3} \lambda_k \leqslant X_k \leqslant 2 \lambda_k \}$. On a donc :
\begin{equation} \label{incl}
C_k \cap A_{k-1}^c \cap I_k \subset A_k \cup B_k
\end{equation}

Remarquons maintenant que $\bigcap_{k \leqslant N} C_k \subset \bigcup_{k \leqslant N} A_k \cup \bigcup_{k \leqslant N} A_k^c \cap C_k$, ce qui donne :
\begin{equation} \label{cups}
 P\left( \bigcup_{k \leqslant N} A_k \right) \geqslant P\left(\bigcap_{k \leqslant N} C_k \right) - P\left( \bigcup_{k \leqslant N} A_k^c \cap C_k \right)
\end{equation}

\begin{Lemme} \label{ref}
Pour tout $N \in \mathbb{N}$, on a
\begin{equation*} \label{rec}
P\left( \bigcup_{k \leqslant N} A_k^c \cap C_k \right) \leqslant \sum _{k \leqslant N} P(B_k) - \left( 1 - \exp (\alpha (n) ) \right) ^N
\end{equation*}
où $\alpha(n) = -\kappa h(R_{F(n)})L(R_{F(n)})$ obtenu dans (\ref{pik}).
\end{Lemme}
\begin{proof}
On procède par récurrence sur $N \in \mathbb{N}$ :
la propriété est vraie en $N=0$, et si elle est vraie pour $N < F(n)$ alors :
\begin{eqnarray}
 P\left( \bigcup_{k \leqslant N+1} A_k^c \cap C_k \right) & \leqslant & E[1_{\bigcup_{k \leqslant N} A_k^c \cap C_k} P(A_N^c \cap A_{N+1}^c \cap C_{N+1} \mid \mathcal{G}_{N} ) ]
 \nonumber \\ & \leqslant & E[1_{\bigcup_{k \leqslant N} A_k^c \cap C_k} \left(  P(B_{N+1} \mid \mathcal{G}_{N} ) + 1-P(I_{N+1} \mid \mathcal{G}_{N}) \right) ]
 \nonumber
\end{eqnarray}
où on a utilisé l'inclusion $A_{N}^c \cap A_{N+1}^c \cap C_{N+1} \subset I_{N+1}^c \cup B_{N+1}$, qui découle de l'inclusion fondamentale (\ref{incl}) au rang $N+1$.
\begin{eqnarray}
 P\left( \bigcup_{k \leqslant N+1} A_k^c \cap C_k \right) & \leqslant & P(B_{N+1}) + (1-\exp (\alpha (n) ) ) P\left( \bigcup_{k \leqslant N} A_k^c \cap C_k \right)
 \nonumber \\ & \leqslant & P(B_{N+1}) +  \sum _{k \leqslant N} P(B_k) - (1-\exp (\alpha (n) ) )\left( 1 - \exp (\alpha (n) ) \right) ^N
 \nonumber \\ & \leqslant & \sum _{k \leqslant N+1} P(B_k) - \left( 1 - \exp (\alpha (n) ) \right) ^{N+1}
 \nonumber
\end{eqnarray}
ce qui achève la preuve.
\end{proof}

En appliquant le lemme \ref{rec} à (\ref{cups}), avec $N=F(n)$, on trouve :
\begin{equation} \label{fin}
 P\left( \bigcup_{k \leqslant F(n)} A_k \right) \geqslant 1 - \sum_{k \leqslant F(n)} \left( P(B_k) + P(C_k^c) \right) - (1 - \exp ( - \kappa h(R_{F(n)}) L(R_{F(n)}) ) ) ^{F(n)}
\end{equation}

On va alors choisir $F(n) = \lfloor \frac{n}{h(n)} \rfloor$, en effet, pour ce choix on a :
\begin{itemize}
 \item $P(B_k)$ décroît plus vite que toute puissance de $n$ pour $\gamma$ suffisamment grand d'après un résultat de \cite{ref} donc $\sum_{k \leqslant F(n)} P(B_k) \to_{n \to \infty} 0$.
 \item $P(C_k^c) \leqslant \frac{C}{\lambda_k}$ d'après les résultats annexes \ref{deuxmoy} et \ref{23moy}. Comme $\lambda_k \geqslant \lambda_n \sim h(n) n^{d-1}$ on a :
 \begin{equation*}
 \sum_{k \leqslant F(n)} P(C_k^c) \leqslant \frac{n}{h(n)} \frac{C}{h(n)n^{d-1}} \underset{n\to+\infty}{\longrightarrow} 0
 \end{equation*}
 \item $R_{F(n)}$ est d'ordre $n$ donc
  \begin{equation*}
  (1 - \exp ( - \kappa h(R_{F(n)}) L(R_{F(n)}) ) ) ^{F(n)} \leqslant \exp\left(-\frac{n}{h(n)} \exp (- \kappa ' h(n) L(n) ) \right)
  \end{equation*}
  Or, comme $h(n) =\sqrt{\alpha \log (n) }$ et $L(n)=\sqrt{\gamma \log (n) }$,
  \begin{equation*}
  \frac{n}{h(n)} \exp (- \kappa ' h(n) L(n) ) = \frac{n^{1-\kappa ' \sqrt{\alpha\gamma}}}{\sqrt{\alpha \log (n) }} \underset{n\to+\infty}{\longrightarrow} +\infty
  \end{equation*}
  pourvu qu'on ait pris $\alpha$ assez petit ($\alpha < \frac{1}{\gamma\kappa'^2}$).
\end{itemize}

Avec ce choix, (\ref{fin}) implique alors que 
\begin{equation*} P\left( \bigcup_{k \leqslant F(n)} A_k \right) \underset{n\to+\infty}{\longrightarrow} 1
\end{equation*}
ce qui achève la preuve du théorème \ref{int}.
\newpage

\section{Autres questions}
On expose ici des variantes du problème, pour lesquelles la forme de la région explorée n'est pas encore connue.

\subsection{Plusieurs sources}
On peut décider que les explorateurs partent de deux points sources, espacés d'une distance de l'ordre du rayon théorique de la boule.

\subsection{Décalage vers la droite}
Choisissons de ne plus faire voyager nos $n$ explorateurs selon des marches aléatoires symétriques, mais des marches déséquilibrées vers un côté. Par exemple, sur $\mathbb{Z}^2$, construisons une marche en suivant les probabilités
\[P(\mathrm{haut}) = 1/2, P(\mathrm{bas}) = 1/2, P(\mathrm{gauche}) = 1/2 - \epsilon, P(\mathrm{droite}) = 1/2 + \epsilon , \]
où $\epsilon > 0$.

Voici un exemple de région obtenue :
\begin{figure}[H]
 \centering
   \includegraphics[scale=0.8]{drift.jpg}
   \caption{Région obtenue pour des marches déséquilibrées}
\end{figure}

Il semble que la déviation à droite dans ce cas soit de l'ordre de $n^{2/3}$, comme le suggère la courbe suivante où l'on trace l'évolution du logarithme de cette déviation et une courbe référence de pente $2/3$ :
\begin{figure}[H]
 \centering
   \includegraphics[scale=0.5]{log.jpg}
   \caption{Log de la distance maximale à l'origine (en noir) et droite de pente 2/3 (en bleu)}
\end{figure}

De plus, on peut avancer l'argument suivant : 
\appendix 

\section{Propriétés annexes}
\subsection{Découpage d'une variable de Poisson}
\begin{Prop}
\label{indé}
Si $\{U_n,n\in\mathbb{N}\}$ est une suite i.i.d. de variables aléatoires de loi $U$ à valeur dans $E$, $X$ une variable de Poisson indépendante des $U_n$ de paramètre $\lambda$, $\{E_i,i\in\{1,\dots,n\}\}$ une partition de E, et si on pose 
\begin{equation*}
 X_i=\sum_{n\leqslant X}{1_{U_n\in E_i}}
\end{equation*}
alors les $X_i$ sont des variables de Poisson indépendantes de paramètre $\lambda P(U \in E_i )$
\end{Prop}
\label{demindé}
\begin{proof}\ 
Ce sont des variables de Poisson :
\begin{align*}
\mathbb{P}(X_i=k)&=\mathbb{P}(\sum_{n\leqslant X}{1_{U_n\in E_i}=k})
\\&=\sum_{p=k}^{\infty}{\mathbb{P}(1_{X=p}\sum_{n\leqslant p}{1_{U_n \in E_i}=k}})
\\&=\sum_{p=k}^{\infty}{\binom{p}{k}\mathbb{P}(X=p)\mathbb{P}(U_1\in E_i)...\mathbb{P}(U_k\in E_i)\mathbb{P}(U_{k+1}\notin E_i)...\mathbb{P}(U_{p}\notin E_i)}
\\&=\frac{e^{-\lambda}}{k!}\mathbb{P}(U\in E_i)^k\sum_{p=k}^{\infty}{\frac{(1-\mathbb{P}(U\in E_i))^{p-k}\lambda^{p}}{(p-k)!}}
\\&=\frac{(\lambda\mathbb{P}(U\in E_i))^k}{k!}e^{-\lambda\mathbb{P}(U\in E_i)}
\end{align*}

Elles sont indépendantes :
\begin{align*}
&\mathbb{P}(\bigcap_{i\in I}X_i=k_i)
\\&=\mathbb{P}(\sum_{n\leqslant X}{\bigcap_{i\in I}1_{U_n\in E_i}}=k_i)
\\&=\sum_{p=\sum_{i\in I}k_i}^{\infty}{\prod_{i\in I}\binom{p-\sum_{j\in I,j<i}k_j}{k_i}\mathbb{P}(X=p)\big(\prod_{i\in I}\mathbb{P}(U\in E_i)^{k_i}\big)\mathbb{P}(U\notin\bigcup_{i\in I} E_i)^{p-\sum_{i\in I}k_i}}
\\&=\frac{e^{-\lambda}}{\prod_{i\in I}(k_i!)}\big(\prod_{i\in I}\mathbb{P}(U\in E_i)^{k_i}\big)\sum_{p=\sum_{i\in I}k_i}^{\infty}{\frac{\lambda^p}{(p-\sum_{i\in I}k_i)!}\mathbb{P}(U\notin\bigcup_{i\in I} E_i)^{p-\sum_{i\in I}k_i}}
\\&=\prod_{i\in I}\mathbb{P}(X_i=k_i)
\end{align*}
\end{proof}

\subsection{Estimations sur une variable de Poisson}
\begin{Prop}
 \label{deuxmoy}
 Soit $X_\lambda$ une variable de Poisson de paramètre $\lambda$. On a :
 \begin{equation*}
  P(X_\lambda>2\lambda)=O_{\lambda \to \infty}(\frac1\lambda)
 \end{equation*}
\end{Prop}
\begin{proof}
 On a $E[X_\lambda] = \lambda$ et $V(X_\lambda) = \lambda$. Ainsi,
 \begin{equation*}
  P(X_\lambda> 2 \lambda) \leqslant P(|X_\lambda - E[X_\lambda] | > \lambda)
 \end{equation*}
 et on peut appliquer l'inégalité de Bienaymé-Tchebicheff :
 \begin{eqnarray*}
  P(X_\lambda>2 \lambda) &\leqslant& \frac1{\lambda ^2} V(X_\lambda)
  \\ &\leqslant& \frac1\lambda
 \end{eqnarray*}
\end{proof}

\begin{Prop}
 \label{23moy}
 Soit $X_\lambda$ une variable de Poisson de paramètre $\lambda$. On a :
 \begin{equation*}
  P(X_\lambda<\frac23 \lambda)=O_{\lambda \to \infty}(\frac1\lambda)
 \end{equation*}
\end{Prop}
\begin{proof}
 La preuve est la même que pour la proposition précédente.
\end{proof}

\subsection{Estimation de $b(n)$}
\begin{Lemme}
\label{b(n)}
\begin{enumerate}[i)]
\item Soit $\omega_d$ le volume de la boule unité de $\mathbb{Z}^d$, on a :
 \begin{equation*}
  b(n)\sim\omega_dn^d
 \end{equation*}
\item Soit $a_n$ une suite telle que $a_n\to\infty$ et $a_n=o(n)$, on a :
 \begin{equation*}
   b(n+a_n)-b(n)\sim\omega_da_nn^{d-1}
 \end{equation*}
\end{enumerate}
 \begin{proof}
 \begin{enumerate}[i)]
 \item On note $\lambda$ la mesure de Lebesgue dans $\mathbb{R}^d$. Notons $V(n)=\omega_dn^d$ le volume de la boule de rayon $n$ et $\alpha_d=\sqrt d$ le diamètre d'un cube unité. On peut paver $\mathbb{R}^d$ par des cubes unités dont les centres appartiennent à $\mathbb{Z}^d$. On note $c_z$ un tel cube, avec $z$ son centre. On pose alors :
 \begin{equation*}
  A(n)=\bigcup_{c_z\cap B(0,n)\neq\emptyset}{c_z}
 \end{equation*}
On remarque que $c(n):=\lambda (A(n))= \mathrm{Card} \{z \ \mid \ c_z\cap B(0,n)\neq\emptyset\}$ vérifie :
\begin{align*}
 | c(n)-V(n) | = c(n)-V(n)&\leqslant\lambda(\bigcup_{z \ \mid \ c_z\cap B(0,n)\neq\emptyset,c_z\not\subset B(0,n)}{c_z})
 \\&\leqslant\lambda(B(0,n+\alpha_d))-\lambda(B(0,n-\alpha_d))
 \\&=((n+\alpha_d)^d-(n-\alpha_d)^d)\omega_d
 \\&\sim2\alpha_dn^{d-1}\omega_d
\end{align*}
De même :
\begin{align*}
 | c(n)-b(n) |=c(n)-b(n)&\leqslant \mathrm{Card}(\{z \ \mid \ c_z\cap B(0,n)\neq\emptyset,c_z\not\subset B(0,n)\})
 \\&\sim2\alpha_dn^{d-1}\omega_d
\end{align*}
Et on a donc :
\begin{equation*}
 \normalfont{|}\frac{b(n)}{V(n)}-1\normalfont{|}\leqslant\frac{1}{V(n)}(\normalfont{|}c(n)-b(n)\normalfont{|}+\normalfont{|}c(n)-V(n)\normalfont{|})=O(\frac{1}{n})
\end{equation*}

\item Posons :
 \begin{equation*}
  A'(n)=\bigcup_{c_z\subset B(0,n)}{c_z}
 \end{equation*}
 et $c'(n)=\lambda (A'(n))$
 \begin{align*}
  &b(n+a_n)-b(n)
  \\&=(b(n+a_n)-c(n+a_n))+(c(n+a_n)-c'(n))+(c'(n)-b(n))
  \\&\leqslant c(n+a_n)-c'(n)
  \\&\leqslant c(n+a_n)-V(n+a_n+\alpha_d)+V(n+a_n+\alpha_d)-V(n-\alpha_d)+V(n-\alpha_d)-c'(n)
  \\&\leqslant V(n+a_n+\alpha_d)-V(n-\alpha_d)
  \\&=\omega_d((n+a_n+\alpha_d)^d-(n-\alpha_d)^d)
  \\&=\omega_da_nn^{d-1}+o(\omega_da_nn^{d-1})
 \end{align*}
De même :
 \begin{align*}
  &b(n+a_n)-b(n)
  \\&=(b(n+a_n)-c'(n+a_n))+(c'(n+a_n)-c(n))+(c(n)-b(n))
  \\&\geqslant c'(n+a_n)-c(n)
  \\&\geqslant c'(n+a_n)-V(n+a_n-\alpha_d)+V(n+a_n-\alpha_d)-V(n+\alpha_d)+V(n+\alpha_d)-c(n)
  \\&\geqslant V(n+a_n-\alpha_d)-V(n+\alpha_d)
  \\&=\omega_d((n+a_n-\alpha_d)^d-(n+\alpha_d)^d)
  \\&=\omega_da_nn^{d-1}+o(\omega_da_nn^{d-1})
 \end{align*}
 D'où
 \begin{equation*}
   b(n+a_n)-b(n)\sim\omega_da_nn^{d-1}
 \end{equation*}
 \end{enumerate}
\end{proof}
\end{Lemme}
\newpage

\bibliographystyle{unsrt-fr}
\bibliography{document_final}
\end{document}
